{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 : 의미를 가지는 요소로서는 더 이상 분석할 수 없는 가장 작은 말의 단위\n",
    "### KoNLPy는 시중에 공개된 hannanum, kkma, okt, komoran, mecab 다섯개 형태소 분석기를 한꺼번에 묶어서 편리하게 사용할 수 있도록 한 패키지\n",
    "### okt\n",
    "- morphs(phrase, norm=False, stem=False)\\\n",
    "  Parse phrase to morphemes.\n",
    "- nouns(phrase)  \n",
    "- phrases(phrase)  \n",
    "- pos(phrase, norm=False, stem=False, join=False)\\\n",
    "  매개 변수:\\\n",
    "  norm -- If True, normalize tokens.\\\n",
    "  stem -- If True, stem tokens.\\\n",
    "  join -- If True, returns joined sets of morph and tag\n",
    " \n",
    "- 파싱(Parsing)\n",
    " - 일련의 문자열을 의미있는 token(어휘 분석의 단위)으로 분해하고 그것들로 이루어진 Parse tree를 만드는 과정\n",
    " - 어떤 문장을 분석하거나 문법적 관계를 해석하는 행위\n",
    " - 프로그램을  compile하는 과정에서 특정 프로그래밍 언어가 제시하는 문법을 잘 지켜서 작성하였는지 compiler가 검사하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoNLPy 설치\n",
    "# Java 환경 세팅, JPype1 다운로드 받고 설치(conda -c conda-forge jpype1)\n",
    "# pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting lxml>=4.1.0\n",
      "  Downloading lxml-4.5.0-cp37-cp37m-win_amd64.whl (3.7 MB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-0.7.2-cp37-cp37m-win_amd64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from konlpy) (1.18.1)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from konlpy) (0.4.3)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading tweepy-3.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.14.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Installing collected packages: lxml, JPype1, beautifulsoup4, tweepy, konlpy\n",
      "Successfully installed JPype1-0.7.2 beautifulsoup4-4.6.0 konlpy-0.5.2 lxml-4.5.0 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
      "\n",
      "['항공기', '체계', '종합', '개발', '경험']\n",
      "\n",
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석으로 문장을 단어로 분할\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs('단독입찰보다 복수입찰의 경우'))\n",
    "print()\n",
    "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAI는'))\n",
    "print()\n",
    "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))\n",
    "print()\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ'))\n",
    "print()\n",
    "# norm 옵션 : '되나욬' 처럼 작성시 '되나요'로 변환\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))\n",
    "print()\n",
    "# stem 옵션 : '되나욬' 처럼 작성시 '되다'로 원형을 찾아줌\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
    "print()\n",
    "# join 옵션 : joined sets of morph and tag\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True, join=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. 아래 문장을 적절한 Okt 옵션을 사용해서 형태소 분석 하세요.\n",
    "Okt 옵션 : morphs, nouns, phrases, normalize, pos(norm, stem, join)\n",
    "\n",
    "- '나는 오늘 방콕에 가고싶다.' (명사만 추출)\n",
    "- '나는 오늘 방콕에 갔다.' (원형만 추출)\n",
    "- '친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.' (형태소 추출)\n",
    "- '나는 오늘도 장에 가고싶다.' (형태소/태그 추출) \n",
    "- '나는 오늘 장에 가고싶을깤ㅋㅋ?' (정규화, 원형 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사\n",
      "['나', '오늘', '방콕']\n",
      "원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('방콕', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('.', 'Punctuation')]\n",
      "형태소\n",
      "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
      "형태소/태그\n",
      "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가다/Verb', './Punctuation']\n",
      "정규화, 원형\n",
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# okt.morphs, okt.pos\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "malist1 = okt.nouns('나는 오늘 방콕에 가고싶다.')\n",
    "malist2 = okt.pos('나는 오늘 방콕에 갔다.', norm=True, stem=True)\n",
    "malist3 = okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.')\n",
    "malist4 = okt.pos('나는 오늘도 장에 가고싶다.', norm=True, stem=True, join=True)\n",
    "malist5 = okt.pos('나는 오늘 장에 가고싶을깤ㅋㅋ?', norm=True, stem=True)\n",
    "print('명사')\n",
    "print(malist1)\n",
    "print('원형')\n",
    "print(malist2)\n",
    "print('형태소')\n",
    "print(malist3)\n",
    "print('형태소/태그')\n",
    "print(malist4)\n",
    "print('정규화, 원형')\n",
    "print(malist5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p  = 웹 페이지를 분석하는 것\n",
      "p  = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# beautifulsoup4 : 파이썬으로 스크레이핑할 때 필요한 라이브러리\n",
    "# 간단하게 HTML과 XML에서 정보를 추출. 다운로드 기능은 없음\n",
    "# !pip install beautifulsoup4\n",
    "\n",
    "# BeautifulSoup 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "# 분석하고 싶은 HTML \n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1>스크레이핑이란?</h1>\n",
    "  <p>웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "# HTML 분석하기 (분석기 종류 : html.parser)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# 원하는 부분 추출하기 \n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "# 요소의 글자 출력하기 \n",
    "print(\"h1 = \" + h1.string)\n",
    "print(\"p  = \" + p1.string)\n",
    "print(\"p  = \" + p2.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title=스크레이핑이란?\n",
      "#body=웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 찾는 방법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1 id=\"title\">스크레이핑이란?</h1>\n",
    "  <p id=\"body\">웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find() 메서드로 원하는 부분 추출하기 \n",
    "# id를 지정해 요소 추출\n",
    "title = soup.find(id=\"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "# 텍스트 부분 출력하기\n",
    "print(\"#title=\" + title.string)\n",
    "print(\"#body=\" + body.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup \n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "# HTML 분석하기 \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "# find_all() 메서드로 추출하기 \n",
    "links = soup.find_all(\"a\")\n",
    "# 링크 목록 출력하기 \n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'melon', 'pear', 'plum']\n",
      "['apple', 'banana', 'melon', 'pear', 'plum']\n",
      "\n",
      "['melon', 'banana', 'plum', 'apple', 'pear']\n",
      "['pear', 'apple', 'banana', 'plum', 'melon']\n",
      "\n",
      "dict_items([('apple', 2), ('banana', 1), ('melon', 0), ('pear', 3), ('plum', 1)])\n",
      "[('apple', 2), ('banana', 1), ('melon', 0), ('pear', 3), ('plum', 1)]\n",
      "[('melon', 0), ('banana', 1), ('plum', 1), ('apple', 2), ('pear', 3)]\n"
     ]
    }
   ],
   "source": [
    "# { k : v} dict 사전 - 정렬(sort)\n",
    "# 사전에서 제공되는 items() method를 사용하여 튜플(tuple) 항목들로 이뤄진 목록을\n",
    "# 정렬하면 사전의 key로 정렬한 것과 동일\n",
    "\n",
    "fruits = { 'apple': 2, 'banana' : 1, 'melon' : 0, 'pear' : 3, 'plum' : 1}\n",
    "print(sorted(fruits)) # 키를 기준으로 정렬\n",
    "print(sorted(fruits.keys()))\n",
    "print()\n",
    "print(sorted(fruits, key=lambda k : fruits[k])) # 값을 기준\n",
    "print(sorted(fruits, key=lambda k : fruits[k], reverse=True))\n",
    "print()\n",
    "print(fruits.items())\n",
    "print(sorted(fruits.items()))\n",
    "# lambda 함수에 t와 t[1]을 사용. t는 fruits.items()에서 얻은 항목을 의미\n",
    "# t 항목은 ('key', value) 튜플. t[1]은 정렬하는 키를 value로 하라는 의미\n",
    "\n",
    "print(sorted(fruits.items(), key=lambda t : t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "것(644) 그(554) 말(485) 안(304) 소리(196) 길(194) 용이(193) 눈(188) 놈(180) 내(174) 사람(167) 봉(165) 치수(160) 평산(160) 얼굴(156) 거(152) 네(151) 일(149) 이(148) 못(147) 댁(141) 생각(141) 때(139) 강청댁(137) 수(134) 서방(131) 집(131) 나(122) 더(120) 서희(119) 머(116) 어디(112) 마을(111) 최(110) 년(109) 김(99) 칠성(97) 구천이(96) 니(96) 뒤(91) 제(90) 날(90) 아이(88) 하나(84) 녀(83) 두(83) 참판(82) 월(82) 손(81) 임(79) "
     ]
    }
   ],
   "source": [
    "# UTF(Unicode Transformation Format)는 몇 비트단위로사용해서 index를 나타내는가를 의미\n",
    "# UTF-8은 8bit씩 UTF-16은 16bit씩 index를 나타냄\n",
    "# 모든 영어는 1byte만 있으면 256개를 표현할 수 있기 때문에 UTF-16을 쓰면 손해. UTF-8 유리\n",
    "# UTF-8, UTF-16는 모두 unicode의 문자 index를 나타내는 방법이므로 상호 변환 가능\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기 (BEXX0003 작업폴더 복사)\n",
    "fp = codecs.open(\"./dataset/BEXX0003.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body > text\")\n",
    "text = body.getText()\n",
    "text\n",
    "# 텍스트를 한 줄씩 처리하기 --- (※2)\n",
    "okt = Okt()\n",
    "word_dic = {}\n",
    "lines = text.split(\"\\n\")\n",
    "for line in lines:\n",
    "    malist = okt.pos(line)\n",
    "    for word in malist:\n",
    "        if word[1] == \"Noun\": #  명사 확인하기 --- (※3)\n",
    "            if not (word[0] in word_dic):\n",
    "                word_dic[word[0]] = 0\n",
    "            word_dic[word[0]] += 1 # 카운트하기\n",
    "# 많이 사용된 명사 출력하기 --- (※4)\n",
    "keys = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)\n",
    "for word, count in keys[:50]:\n",
    "    print(\"{0}({1}) \".format(word, count), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장을 벡터로 변환하기\n",
    "- 단계 \n",
    " - 코퍼스 생성 : 데이터 내려받기 - XML을 일반 텍스트로 변환 - 형태소 분석, 단어로 구분\n",
    " - 코퍼스를 이용하여 Word2Vec로 모델 생성하며 단어를 벡터로 변환하고 모델 저장\\\n",
    "   매개변수 : sg 알고리즘 선택(1=Skip-gram, 0=CBOW), size (벡터의 차원 설정), window (학습할 단어를 연관시킬 앞뒤의 단어 수)\n",
    " - 모델을 읽어 들여 계산에 사용\n",
    "- Word2Vec : 구글의 토머스 미코로프가 만든 방법으로 딥러닝 기술을 사용하여 단어를 벡터로 만드는 방법으로 대량의 문장을 기반으로 학습하고 단어를 벡터로 변환\n",
    " - 단어는 그 주변의 단어들과 관계가 있다.\n",
    " - 특정 단어의 유의어, 반의어를 추출할 수 있다.\n",
    " - 단어를 선형으로 나타낼 수 있다.\n",
    " - 자연 언어 처리에 활용할 수 있다.\n",
    " - 추천 분류 시스템에 다양하게 사용될 수 있다.\n",
    "- Word2Vec 알고리즘\n",
    " - Skip-gram\n",
    " - CBOW \n",
    " \n",
    "※ 코퍼스(Corpus) : 모델을 만들기 위한 대량의 띄어쓰기로 구분한 데이터를 포함. 컴퓨터로 검색이 가능한 대량의 언어 데이터\\\n",
    "※ Word2Vec는 띄어쓰기로 구분된 단어를 학습시키는 이론. 형태소 분석을 사용해 단어들을 정규화해서 추출하고 이를 기반으로 띄어쓰기로 구분한 데이터를 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim 설치 : 자연 언어 처리를 위한 라이브러리로 Word2Vec 기능 포함\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기\n",
    "fp = codecs.open(\"./dataset/BEXX0003.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body > text\")\n",
    "text = body.getText()\n",
    "# 텍스트를 한 줄씩 처리하기\n",
    "okt = Okt()\n",
    "results = []\n",
    "lines = text.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    # 형태소 분석하기\n",
    "    # 단어의 기본형 사용\n",
    "    malist = okt.pos(line, norm=True, stem=True)\n",
    "    r = []\n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 등은 대상에서 제외 \n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "        # join(리스트)는 구분자 문자열과 문자열 리스트의 요소를 연결\n",
    "        # strip()은 문자열에서 양쪽에 있는 연속된 모든 공백을 삭제    \n",
    "    rl = (\" \".join(r)).strip()\n",
    "    results.append(rl)\n",
    "#     print(rl)\n",
    "# 파일로 출력하기 \n",
    "wakati_file = './dataset/toji.wakati'\n",
    "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(results))\n",
    "# Word2Vec 모델 만들기 \n",
    "data = word2vec.LineSentence(wakati_file)\n",
    "# size : Dimensionality of the word vectors.\n",
    "# window : Maximum distance between the current and predicted word within a sentence\n",
    "# min_count : Ignores all words with total frequency lower than this\n",
    "# sg : 1 for skip-gram; otherwise CBOW\n",
    "# hs=1 : hierarchical softmax will be used for model training\n",
    "model = word2vec.Word2Vec(data, \n",
    "    size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "\n",
    "model.save(\"./dataset/toji.model\")\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load('./dataset/toji.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('꾼', 0.9090118408203125),\n",
       " ('골골', 0.8798800110816956),\n",
       " ('벼슬길', 0.8707199692726135),\n",
       " ('단명', 0.8679108619689941),\n",
       " ('봇짐', 0.8655251264572144),\n",
       " ('석', 0.8653677701950073),\n",
       " ('우쩌겄소', 0.8651825785636902),\n",
       " ('기생', 0.8646311163902283),\n",
       " ('커서', 0.8580085039138794),\n",
       " ('대가', 0.8565864562988281)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_similar() : 유사한 단어 추출\n",
    "model.wv.most_similar(positive=['땅'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('제', 0.8321410417556763),\n",
       " ('구석', 0.7668663263320923),\n",
       " ('날', 0.7535632252693176),\n",
       " ('매', 0.7471103668212891),\n",
       " ('열', 0.746376633644104),\n",
       " ('말짱', 0.745918869972229),\n",
       " ('이지마', 0.7413662075996399),\n",
       " ('점심', 0.7408493757247925),\n",
       " ('그까짓', 0.7381689548492432),\n",
       " ('돌아가다', 0.7366471290588379)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['집'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
