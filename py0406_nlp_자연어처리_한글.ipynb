{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from konlpy) (0.4.3)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-0.7.2-cp37-cp37m-win_amd64.whl (1.3 MB)\n",
      "Collecting tweepy>=3.7.0\n",
      "  Downloading tweepy-3.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting lxml>=4.1.0\n",
      "  Downloading lxml-4.5.0-cp37-cp37m-win_amd64.whl (3.7 MB)\n",
      "Collecting beautifulsoup4==4.6.0\n",
      "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from konlpy) (1.18.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.14.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
      "Installing collected packages: JPype1, tweepy, lxml, beautifulsoup4, konlpy\n",
      "Successfully installed JPype1-0.7.2 beautifulsoup4-4.6.0 konlpy-0.5.2 lxml-4.5.0 tweepy-3.8.0\n"
     ]
    }
   ],
   "source": [
    "# KoNLPy 설치\n",
    "# 설치 전에 자바 환경 세팅되어 있어야 함\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
      "\n",
      "['항공기', '체계', '종합', '개발', '경험']\n",
      "\n",
      "['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행']\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나요', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "\n",
      "['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소 분석으로 문장을 단어로 분할\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.morphs('단독입찰보다 복수입찰의 경우'))\n",
    "print() #['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n",
    "\n",
    "print(okt.nouns('유일하게 항공기 체계 종합개발 경험을 갖고 있는 KAL는'))\n",
    "print() #['항공기', '체계', '종합', '개발', '경험']\n",
    "\n",
    "print(okt.phrases('날카로운 분석과 신뢰감 있는 진행으로'))\n",
    "print() #['날카로운 분석', '날카로운 분석과 신뢰감', '날카로운 분석과 신뢰감 있는 진행', '분석', '신뢰', '진행'\n",
    "\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ'))\n",
    "print()\n",
    "\n",
    "# norm 옵션: '되나욬'처럼 작성 시 '되나요'로 변환\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True))\n",
    "print()\n",
    "\n",
    "# stem 옵션: '되나욬'처럼 작성 시 '되다'로 원형을 찾아줌\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True))\n",
    "print() #[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되다', 'Verb'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=False, stem=True))\n",
    "print() #[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('되나욬', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
    "\n",
    "# join 옵션: joined sets of morphs and tag\n",
    "print(okt.pos('이것도 되나욬ㅋㅋ', norm=True, stem=True, join=True))\n",
    "print() #['이/Determiner', '것/Noun', '도/Josa', '되다/Verb', 'ㅋㅋ/KoreanParticle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 아래 문장을 적절한 Okt 옵션을 사용하여 형태소 분석\n",
    "\n",
    "Okt 옵션: morphs, nouns, phrases, normalize, pos(norm, stem, join)\n",
    "\n",
    "- 명사만 추출\n",
    "'나는 오늘 방콕에 가고 싶다.'\n",
    "\n",
    "- 원형만 추출\n",
    "'나는 오늘 방콕에 갔다.'\n",
    "\n",
    "- 형태소 추출\n",
    "'친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.'\n",
    "\n",
    "- 형태소/태그 추출\n",
    "'나는 오늘도 장에 가고싶다.'\n",
    "\n",
    "- 정규화, 원형 추출\n",
    "'나는 오늘도 장에 가고싶을깤ㅋㅋ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '오늘', '방콕']\n"
     ]
    }
   ],
   "source": [
    "# 명사만 추출\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.nouns('나는 오늘 방콕에 가고싶다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('방콕', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# 원형만 추출\n",
    "print(okt.pos('나는 오늘 방콕에 갔다.', norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친절한', '코치', '와', '재미있는', '친구', '들', '이', '있는', '도장', '에', '가고', '싶다', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 형태소 추출(어폐가 있음)\n",
    "print(okt.morphs('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.'))\n",
    "print()\n",
    "#print(okt.pos('친절한 코치와 재미있는 친구들이 있는 도장에 가고 싶다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나/Noun', '는/Josa', '오늘/Noun', '도/Josa', '장/Noun', '에/Josa', '가다/Verb', './Punctuation']\n"
     ]
    }
   ],
   "source": [
    "# 형태소/태그 추출\n",
    "print(okt.pos('나는 오늘도 장에 가고싶다.', norm=True, stem=True, join=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('나', 'Noun'), ('는', 'Josa'), ('오늘', 'Noun'), ('도', 'Josa'), ('장', 'Noun'), ('에', 'Josa'), ('가다', 'Verb'), ('ㅋㅋ', 'KoreanParticle'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "#  정규화, 원형 추출\n",
    "print(okt.pos('나는 오늘도 장에 가고싶을깤ㅋㅋ?', norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\workspace'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "것(644) 그(554) 말(485) 안(304) 소리(196) 길(194) 용이(193) 눈(188) 놈(180) 내(174) 사람(167) 봉(165) 치수(160) 평산(160) 얼굴(156) 거(152) 네(151) 일(149) 이(148) 못(147) 댁(141) 생각(141) 때(139) 강청댁(137) 수(134) 서방(131) 집(131) 나(122) 더(120) 서희(119) 머(116) 어디(112) 마을(111) 최(110) 년(109) 김(99) 칠성(97) 구천이(96) 니(96) 뒤(91) 제(90) 날(90) 아이(88) 하나(84) 녀(83) 두(83) 참판(82) 월(82) 손(81) 임(79) "
     ]
    }
   ],
   "source": [
    "# UTF-8은 몇 비트단위로 사용해서 index를 나타내는가를 의미\n",
    "# UTF-8은 8비트씩 UTF-16은16비트씩 index를 나타냄\n",
    "# 모든 영어는 1byte만 있으면 256개를 표현 가능, UTF-16 쓰면 손해\n",
    "# UTF-8, UTF-16은 모두 unicode 문자 index를 나타내는 바법이므로 상호 변환 가능\n",
    "\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기 (BEXX0003 작업폴더 복사)\n",
    "# 박경리 토지\n",
    "fp = codecs.open(\"./dataset/BEXX0003.txt\", \"r\", encoding = \"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body > text\")\n",
    "text = body.getText()\n",
    "\n",
    "\n",
    "# 텍스트를 한 줄씩 처리하기 --- (※2)\n",
    "okt = Okt()\n",
    "word_dic = {}\n",
    "lines = text.split(\"\\n\")\n",
    "\n",
    "# word 1이 품사, word 0이 단어\n",
    "for line in lines:\n",
    "    malist = okt.pos(line)\n",
    "    for word in malist:\n",
    "        #  명사 확인하기 --- (※3)\n",
    "        if word[1] == \"Noun\":\n",
    "            if not (word[0] in word_dic):\n",
    "                # 처음 나오는 단어면 0을 주고\n",
    "                word_dic[word[0]] = 0\n",
    "                #print(word_dic)\n",
    "            # 이후 다시 나오면 개수만큼 카운트하기\n",
    "            word_dic[word[0]] += 1\n",
    "\n",
    "# 많이 사용된 명사 출력하기 --- (※4)\n",
    "keys = sorted(word_dic.items(), key=lambda x:x[1], reverse=True)\n",
    "for word, count in keys[:50]:\n",
    "    print(\"{0}({1}) \".format(word, count), end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장을 벡터로 변환하기\n",
    "- 단계 \n",
    " - corpus (말뭉치) 생성 : 데이터 내려받기 - XML을 일반 텍스트로 변환 - 형태소 분석, 단어로 구분\n",
    " - 코퍼스를 이용하여 Word2Vec 로 모델 생성하며 단어를 벡터로 변환하고 모델 저장\\\n",
    "   매개변수 : sg 알고리즘 선택(1=Skip-gram, 0=CBOW)(#1이면~, 아니면~), size (벡터의 차원 설정), window (학습할 단어를 연관시킬 앞뒤의 단어 수)\n",
    " - 모델을 읽어 들여 계산에 사용\n",
    " \n",
    "- Word2Vec : 구글의 토머스 미코로프가 만든 방법으로 딥러닝 기술을 사용하여 단어를 벡터로 만드는 방법으로 대량의 문장을 기반으로 학습하고 단어를 벡터로 변환\n",
    " - 단어는 단독으로 존재하지 않고 그 주변의 단어들과 관계가 있다.\n",
    " - 특정 단어의 유의어, 반의어를 추출할 수 있다.\n",
    " - 단어를 선형으로 나타낼 수 있다.\n",
    " - 자연 언어 처리에 활용할 수 있다.\n",
    " - 추천 분류 시스템에 다양하게 사용될 수 있다.\n",
    " \n",
    "- Word2Vec 알고리즘\n",
    " - Skip-gram\n",
    " - CBOW \n",
    " \n",
    "※ 코퍼스(Corpus) : 모델을 만들기 위한 대량의 띄어쓰기로 구분한 데이터를 포함. 컴퓨터로 검색이 가능한 대량의 언어 데이터\n",
    "\n",
    "※ Word2Vec는 띄어쓰기로 구분된 단어를 학습시키는 이론. 형태소 분석을 사용해 단어들을 정규화해서 추출하고 이를 기반으로 띄어쓰기로 구분한 데이터를 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.36)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.27.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.36 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.36)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.11.2)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-cloud-storage->smart-open>=1.8.1->gensim) (1.3.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from botocore<1.16.0,>=1.15.36->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from botocore<1.16.0,>=1.15.36->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (4.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (46.1.3.post20200330)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.1.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.8)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (3.11.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (1.51.0)\n",
      "Requirement already satisfied: pytz in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2019.3)\n"
     ]
    }
   ],
   "source": [
    "# Gensim 설치(Gensin에서 Word2Vec 기능 제공)\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# utf-16 인코딩으로 파일을 열고 글자를 출력하기 (BEXX0003 작업폴더 복사)\n",
    "# 박경리 토지\n",
    "fp = codecs.open(\"./dataset/BEXX0003.txt\", \"r\", encoding = \"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body > text\")\n",
    "text = body.getText()\n",
    "\n",
    "\n",
    "# 텍스트를 한 줄씩 처리하기\n",
    "okt = Okt()\n",
    "results = []\n",
    "lines = text.split(\"\\r\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    # 형태소 분석하기\n",
    "    # 단어의 기본형 사용\n",
    "    malist = okt.pos(line, norm=True, stem=True)\n",
    "    r= []\n",
    "    for word in malist:\n",
    "        # 어미/조사/구두점 등은 대상에서 제외(불용어 제외)\n",
    "        # word 1은 품사, 0은 단어\n",
    "        if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            r.append(word[0])\n",
    "        # join(리스트)는 구분자 문자열과 문자열 리스트의 요소를 연결\n",
    "        # strip()은 문자열에서 양쪽에 있는 연속된 모든 공백을 삭제    \n",
    "    rl = (\" \".join(r)).strip()\n",
    "    results.append(rl)\n",
    "    #print(rl)\n",
    "\n",
    "# 파일로 출력하기\n",
    "wakati_file = './dataset/toji.wakati'\n",
    "with open(wakati_file, 'w', encoding='utf-8') as fp:\n",
    "    fp.write(\"\\n\".join(results))\n",
    "\n",
    "# Word2Vec 모델 만들기\n",
    "data = word2vec.LineSentence(wakati_file)\n",
    "\n",
    "model = word2vec.Word2Vec(data, size=200, window=10, hs=1, min_count=2, sg=1)\n",
    "# size : Dimensionality of the word vectors.\n",
    "# window : Maximum distance between the current and predicted word within a sentence\n",
    "# min_count : Ignores all words with total frequency lower than this\n",
    "# sg : 1 for skip-gram; otherwise CBOW\n",
    "# hs=1 : hierarchical softmax will be used for model training\n",
    "\n",
    "model.save('./dataset/toji.model')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load('./dataset/toji.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('꾼', 0.910788893699646),\n",
       " ('굶주리다', 0.8900405764579773),\n",
       " ('벼슬길', 0.8882701992988586),\n",
       " ('원귀', 0.8862684369087219),\n",
       " ('장차', 0.8854167461395264),\n",
       " ('봇짐', 0.8836709856987),\n",
       " ('기생', 0.8834049701690674),\n",
       " ('일가', 0.883205771446228),\n",
       " ('대가', 0.8825960755348206),\n",
       " ('된장', 0.8803859949111938)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_similar() : 유사한 단어 추출\n",
    "model.wv.most_similar(positive=['땅'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('구석', 0.8157641887664795),\n",
       " ('제', 0.8012774586677551),\n",
       " ('이지마', 0.7581266164779663),\n",
       " ('말짱', 0.7578853368759155),\n",
       " ('남정', 0.7545551061630249),\n",
       " ('해', 0.751640796661377),\n",
       " ('점심', 0.7484650611877441),\n",
       " ('돌아가다', 0.7378664612770081),\n",
       " ('날', 0.7354258298873901),\n",
       " ('나선', 0.7350877523422241)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['집'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('빈말', 0.9727845191955566),\n",
       " ('본', 0.9721720814704895),\n",
       " ('셋', 0.9719869494438171),\n",
       " ('쓰겄', 0.970746636390686),\n",
       " ('일색', 0.9689548015594482),\n",
       " ('어찌', 0.9688628315925598),\n",
       " ('부동', 0.9687970280647278),\n",
       " ('보이', 0.9685132503509521),\n",
       " ('딱하다', 0.9680392742156982),\n",
       " ('고향', 0.9672443866729736)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['부자'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
