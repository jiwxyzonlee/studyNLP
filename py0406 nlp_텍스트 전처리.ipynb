{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP, 텍스트 분석\n",
    "- Natural Language Processing\n",
    ": 기계가 인간의 언어를 이해하고 해석하는 데 중점\n",
    "기계번역, 질의 응답 시스템"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 정규화\n",
    ": 클렌징, 토큰화, 필터링/스톱워드제거/철자수정, Stemming/Lemmatization\n",
    "\n",
    "<자연어 관련 용어>\n",
    "> \n",
    "> 말뭉치 corpus\n",
    "> : 텍스트 문서의 집합\n",
    ">\n",
    "> Token\n",
    "> : 뜻을 가지는 최소한의 단위, 단어처럼 의미를 가지는 요소\n",
    ">\n",
    "> 형태소 Morphemes\n",
    "> : 의미를 가지는 언어 요소에서 최소 단위\n",
    "> - 의미를 살릴 수 있는 최소 단위, 더 분석하면 뜻이 없어지는 말의 단위\n",
    ">\n",
    "> 품사 POS\n",
    ">\n",
    "> 불용어 Stopword\n",
    "> : 조사, 접사와 같이 자주 나타나지만 실제 의미에 기여하지 못하는 단어(문법적, 의존적)\n",
    ">\n",
    "> 어간 추출 Stemming\n",
    "> : 어간만 추출하는 것을 의미(running, runs, run -> run)\n",
    ">\n",
    "> 음소표기법 Lemmatization\n",
    ">: 앞뒤 문맥을 보고 단어를 식별하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\nlp_python\\lib\\site-packages (from nltk) (1.14.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449913 sha256=f09f759bb9e433eed7693bb1f25fcfcfff0e2cb64baba52e60eac53e8d36cd79\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\48\\8b\\7f\\473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모듈 불러오기\n",
    "import nltk\n",
    "\n",
    "# 필요한 punkt 다운로드\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everyewhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes']\n",
      "\n",
      "type(sentences):  <class 'list'> \n",
      "len(sentences):  3\n"
     ]
    }
   ],
   "source": [
    "# 문장 토큰화(sent_tokenize): 마침표, 개행문자(\\n), 정규표현식\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everyewhere its all around us, here even in this room. \\\n",
    "            You can see it out your window or on your television. \\\n",
    "            You feel it when you go to work, or go to church or pay your taxes'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(sentences); print()\n",
    "\n",
    "print('type(sentences): ', type(sentences), '\\nlen(sentences): ', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everyewhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n",
      "\n",
      "type(words):  <class 'list'> \n",
      "len(words):  15\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화(word_tokenize): 공백, 콤마, 마침표, 개행문자, 정규표현식\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everyewhere its all around us, here even in this room.'\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(words); print()\n",
    "\n",
    "print('type(words): ', type(words), '\\nlen(words): ', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everyewhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes']]\n",
      "\n",
      "type(word_tokens):  <class 'list'> \n",
      "len(word_tokens):  3\n"
     ]
    }
   ],
   "source": [
    "# 문서에 대해서 모든 단어를 토큰화\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # 문장별 분리 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "print(word_tokens); print()\n",
    "\n",
    "print('type(word_tokens): ', type(word_tokens), '\\nlen(word_tokens): ', len(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "# 스톱 워드 제거: is, the, all, a, will과 같이 문백적으로 큰 의미가 없는 단어를 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stopwords 개수:  179\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# NLTK의 english stopwords 개수 확인\n",
    "print('영어 stopwords 개수: ', len(nltk.corpus.stopwords.words('english')))\n",
    "# 말뭉치 corpus : 텍스트 문서의 집합\n",
    "print()\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matrix']\n",
      "['matrix', 'everyewhere']\n",
      "['matrix', 'everyewhere', 'around']\n",
      "['matrix', 'everyewhere', 'around', 'us']\n",
      "['matrix', 'everyewhere', 'around', 'us', ',']\n",
      "['matrix', 'everyewhere', 'around', 'us', ',', 'even']\n",
      "['matrix', 'everyewhere', 'around', 'us', ',', 'even', 'room']\n",
      "['matrix', 'everyewhere', 'around', 'us', ',', 'even', 'room', '.']\n",
      "['see']\n",
      "['see', 'window']\n",
      "['see', 'window', 'television']\n",
      "['see', 'window', 'television', '.']\n",
      "['feel']\n",
      "['feel', 'go']\n",
      "['feel', 'go', 'work']\n",
      "['feel', 'go', 'work', ',']\n",
      "['feel', 'go', 'work', ',', 'go']\n",
      "['feel', 'go', 'work', ',', 'go', 'church']\n",
      "['feel', 'go', 'work', ',', 'go', 'church', 'pay']\n",
      "['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']\n",
      "\n",
      "[['matrix', 'everyewhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 내답\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "stop_token = []\n",
    "for sttokens in word_tokens:\n",
    "    if sttokens not in stop_words:\n",
    "        stop_token.append(sttokens)\n",
    "\n",
    "print(stop_token)\n",
    "\"\"\"\n",
    "\n",
    "# stopwords를 필터링을 통한 제거\n",
    "# 소문자 변경 필요\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "            print(filtered_words)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print()\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법: Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[work]:  work work work\n",
      "[amuse]:  amus amus amus\n",
      "[happy]:  happy happiest\n",
      "[fancy]:  fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# Stemmer (Lancaster Stemmer)\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print('[work]: ', stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print('[amuse]: ', stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print('[happy]: ', stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print('[fancy]: ', stemmer.stem('fancier'), stemmer.stem('fanciest'))\n",
    "\n",
    "# 품사 고려 못하고 단순하게만 분류 -> 보완 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[amuse]:  amuse amuse amuse\n",
      "[happy]:  happy happy\n",
      "[fancy]:  fancy fancy\n"
     ]
    }
   ],
   "source": [
    "# 분류기\n",
    "# Lemmatization(WordLemmatizer): 정확한 원형 단어를 추출하기 위해 단어 품사를 입력\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print('[amuse]: ', lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print('[happy]: ',lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print('[fancy]: ',lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))\n",
    "\n",
    "#어간 추출 Stemming : 어간만 추출하는 것을 의미(running, runs, run -> run)\n",
    "#음소표기법 Lemmatization : 앞뒤 문맥을 보고 단어를 식별하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 백터화: One-hot encoding\n",
    "bag of words\n",
    "\n",
    ": 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해서 피처를 추출하는 모델\n",
    "- 단점: 문맥 의미 반영 부족, 희소행렬 문제(메모리를 너무 많이 잡아먹는 비효율적인 구조)\n",
    "\n",
    "BOW에서 피처 벡터화\n",
    "\n",
    ": 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
    "\n",
    "피처 벡터화 방식\n",
    "\n",
    ": 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency)기반 벡터화(의미 없는 단어에는 penalty를 부여하는 보완적인 방법)\n",
    "\n",
    "파라미터\n",
    "- max_df: 너무 높은 빈도수 단어 피처 제외 0.9(90% 이상 나온 것 제외)\n",
    "- min_df: 너무 낮은 빈도수 단어 피처 제외\n",
    "- max_features: 피처 개수 제한\n",
    "- stop_words\n",
    "- n_game_range: 튜플 형태, 범위 최솟값, 범위 최댓값 지정\n",
    "- analyzer: 피처 추출을 수행하는 단위(디폴트는 'word') 단어 한 개 할 건지 세 개 할 건지 등\n",
    "- token_patten\n",
    "- tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ndarray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 희소행렬 COO 형식에서 CSR 형식으로 개선됨\n",
    "\n",
    "# 희소행렬 COO 형식: 0이 아닌 데이터만 별도 데이터 배열에 저장하고\n",
    "# 행과 행의 위치를 별도의 배열로 저장\n",
    "# 메모리 절약 방식\n",
    "\n",
    "# 희소 행렬 변환을 위해 scipy sparse 패키지 이용\n",
    "\"\"\"\n",
    "array([[3, 0, 1],\n",
    "       [0, 2, 0]])\n",
    "\"\"\"\n",
    "\n",
    "from scipy import sparse\n",
    "data = np.array([3, 1, 2])\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "\n",
    "print(sparse_coo)\n",
    "\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sparse_coo]\n",
      "   (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "\n",
      "[sparse_csr]\n",
      "   (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 5],\n",
       "       [1, 4, 0, 3, 2, 5],\n",
       "       [0, 6, 0, 3, 0, 0],\n",
       "       [2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 8],\n",
       "       [1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 희소 행렬 -CSR 형식 (메모리를 더 절약)\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "# CSR 형식으로 변환\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('[sparse_coo]\\n', sparse_coo)\n",
    "print()\n",
    "print('[sparse_csr]\\n', sparse_csr)\n",
    "\n",
    "# 다시 matrix 형태로 보기\n",
    "sparse_csr.toarray()\n",
    "\"\"\"\n",
    "array([[0, 0, 1, 0, 0, 5],\n",
    "       [1, 4, 0, 3, 2, 5],\n",
    "       [0, 6, 0, 3, 0, 0],\n",
    "       [2, 0, 0, 0, 0, 0],\n",
    "       [0, 0, 0, 7, 0, 8],\n",
    "       [1, 0, 0, 0, 0, 0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C']\n",
      "\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "\n",
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "\n",
      "[[0. 0. 4.]]\n",
      "\n",
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#DictVectorizer: 문서에서 단어의 사용 빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치를 벡터로 변환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse = False)\n",
    "\n",
    "D = [{'A':1, 'B':2}, {'B':3, 'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "\n",
    "print(v.feature_names_); print()\n",
    "print(v.vocabulary_); print()\n",
    "print(X); print()\n",
    "\n",
    "print(v.transform({'C':4, 'D':3})); print()\n",
    "\n",
    "# C는 존재하는 값으로 4로 바뀌고 D는 없으므로 생성되거나 바뀌지 않음\n",
    "# 근데 A, B는 왜 0으로 됨?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n",
      "\n",
      "[[0 1 0 1 0 0 1 1 0 1]]\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 2 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "# 문서를 토큰 리스트로 변환\n",
    "# 각 문서를 BOW 인코딩 벡터로 변환\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['This is the first document.'\n",
    "          , 'This is the second second document.'\n",
    "          , 'And the third one'\n",
    "          , 'Is this the first document?'\n",
    "          , 'The last document?']\n",
    "\n",
    "vect = CountVectorizer()\n",
    "# fit()는 데이터를 모델에서 학습시킬 때 사용\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print(vect.vocabulary_)\n",
    "#{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'and': 0, 'third': 8, 'one': 5, 'last': 4}\n",
    "# 단어별로 위치 index(단어가 존재하면 숫자가 표시하는 자리에 1표시 -  두 개이면 2 표시)\n",
    "\n",
    "print()\n",
    "print(vect.transform(['This is the second document']).toarray())\n",
    "# transform()은 데이터를 알맞게 변형해줌\n",
    "\n",
    "print()\n",
    "print(vect.transform(['Something completely new.']).toarray())\n",
    "# 사전에 없으므로 0000000000\n",
    "\n",
    "print()\n",
    "print(vect.transform(corpus).toarray())\n",
    "\n",
    "#[[0 1 1 1 0 0 0 1 0 1]\n",
    "# [0 1 0 1 0 0 2 1 0 1] # second 가 두번 반복되므로 2임\n",
    "# [1 0 0 0 0 1 0 1 1 0]\n",
    "# [0 1 1 1 0 0 0 1 0 1]\n",
    "# [0 1 0 0 1 0 0 1 0 0]]\n",
    "\n",
    "# 말뭉치 사전의 중요성\n",
    "\n",
    "# 문맥마다 중요성이 다르므로 이 점을 보완한 것이 TF-IDF\n",
    "# 자주 나와도 문맥에 의미가 없으면 penalty를 부여하여 중요도를 낮춤\n",
    "# 모든 문서에 공통적으로 들어가 있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 penalty를 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords는 문서에서 단어장을 생성할 때 무시할 수 있는 단어(보통 영어의 관사나 접사)\n",
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0, 'second': 1}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# english stopwords 179개 적용(해당되는 것은 불용어이므로 다 제외해버리기)\n",
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 철자 단위로\n",
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택(디폴트로 word)\n",
    "vect = CountVectorizer(analyzer='char').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this is': 12,\n",
       " 'is the': 2,\n",
       " 'the first': 7,\n",
       " 'first document': 1,\n",
       " 'the second': 9,\n",
       " 'second second': 6,\n",
       " 'second document': 5,\n",
       " 'and the': 0,\n",
       " 'the third': 10,\n",
       " 'third one': 11,\n",
       " 'is this': 3,\n",
       " 'this the': 13,\n",
       " 'the last': 8,\n",
       " 'last document': 4}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기를 결정\n",
    "# 모노그램(1-그램)은 토큰 하나만 단어로 사용, 바이그램(2-그램)은 두개의 연결된 토큰을 하나의 단어로 사용\n",
    "vect = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n",
    "vect.vocabulary_\n",
    "\n",
    "#vect = CountVectorizer(ngram_range=(1,2)).fit(corpus)\n",
    "#vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-84-3617d9e49fb6>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-3617d9e49fb6>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    tf-idf(d, t) = tf(d, t)*idf(t)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF(Term Frequency - Inverse Documnent Frequency) 인코딩은 단어를 개수 그대로 카운트하지 않고\n",
    "# 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소\n",
    "# 문서 d(documnet)와 단어 t에 대해 다음과 같이 계산\n",
    "tf-idf(d, t) = tf(d, t)*idf(t)\n",
    "\n",
    "# tf(d, t): term frequency, 특정한 단어의 빈도수\n",
    "# idf(t): inverse document frequency, 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "# n : 전체 문서의 수\n",
    "# df(t): 단어 t를 가진 문서의 수\n",
    "# idf(d, t) = log(n / (1 + df(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf(d, t): term frequency, 특정한 단어의 빈도수\n",
    "\n",
    "idf(t): inverse document frequency, 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "\n",
    "n : 전체 문서의 수\n",
    "\n",
    "df(t): 단어 t를 가진 문서의 수\n",
    "\n",
    "idf(d, t) = log(n / (1 + df(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.24151532, 0.        , 0.28709733, 0.        ,\n",
       "        0.        , 0.85737594, 0.20427211, 0.        , 0.28709733],\n",
       "       [0.55666851, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.55666851, 0.        , 0.26525553, 0.55666851, 0.        ],\n",
       "       [0.        , 0.38947624, 0.55775063, 0.4629834 , 0.        ,\n",
       "        0.        , 0.        , 0.32941651, 0.        , 0.4629834 ],\n",
       "       [0.        , 0.45333103, 0.        , 0.        , 0.80465933,\n",
       "        0.        , 0.        , 0.38342448, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습시킨 후 벡터로 변환하여 array로 표현\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer().fit(corpus)\n",
    "tfidv.transform(corpus).toarrayray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
