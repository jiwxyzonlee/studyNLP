{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 확인 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화 평점 train 데이터 기반으로 다음 사항에 유의하여 감성분석 후 test 데이터를 이용해 최종 감성 예측을 수행하세요. \n",
    "* 네이버 영화 평점 데이터는 http://github.com/e9t/nsmc에서 ratings.txt(전체), ratings._train.txt(학습), rating_test.txt(테스트)를 다운로드 \n",
    "* 데이터 세트 요약 정보\n",
    "\n",
    "  - 칼럼 : id, document\n",
    "  - label : the sentiment class of the review (0:negative, 1: positive)\n",
    "  - ratings.txt : All 200K reviews\n",
    "  - ratings_test : 50K reviews held out for testing\n",
    "  - ratings_train : 150K reviews for training\n",
    "  \n",
    "* 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
    "* 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
    "* 한글 형태소 분석을 통해 형태소 단어로 토큰화(tokenizer 사용자 함수 작성 추천)\n",
    "  - 한글 형태소 엔진은 SNS 분석에 적합한 Okt 클래스를 이용\n",
    "  - morphs() 메서드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체 반환 \n",
    "  - Okt 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2)\n",
    "* 사이킷런의 TfidVectorizor를 이용, TF-IDF 피처 모델을 생성(10분 이상 소요)\n",
    "* DT, RF, LR 을 이용하여 감성 분석 Classification 수행\n",
    "* 3개 분류 모델별 교차 검증 및 Parameter 최적화를 GridSearchCV 를 이용하여 수행(최적 분류 모델 선정)\n",
    "  - DT params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}\n",
    "  - RF params = {'n_estimators':[50,100, 200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}  \n",
    "  - LR params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\\\n",
    "    C는 규제 강도를 조절하는 alpha 값의 역수로 작을 수록 규제 강도가 큼\n",
    "  - param_grid=params , cv=3 ,scoring='accuracy', verbose=1(학습진행 상황 표시) \n",
    "* 학습데이터에 사용된 TfidVectorizer 객체 변수인 tfidf_vect를 이용해 transform()을 테스트 데이터의 document 칼럼에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news_df = pd.read_csv('./dataset/nsmc/ratings_train.txt', sep='\\t')\n",
    "news_df.head()\n",
    "\n",
    "news_df.label.value_counts()\n",
    "\n",
    "x = news_df.iloc[:, :-1]\n",
    "y = news_df.iloc[:, -1]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y\n",
    "    , test_size=0.2\n",
    "    , random_state=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null, 숫자를 공백으로 처리\n",
    "import re\n",
    "x_train = x_train.fillna(\" \")\n",
    "\n",
    "# 숫자는 공백으로 채우기\n",
    "x_train.document = x_train.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
    "x_test = x_test.fillna(\" \")\n",
    "x_test.document = x_test.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphs() 메소드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화하여 list로 변환\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 TfidfVectorizer를 이용, TF-IDF 피처 모델을 생성(10분 소요)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 위에서 만든 tw_tokenizer() 함수를 tokenizer로 사용, ngram_range는 (1,2)\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    tokenizer=tw_tokenizer\n",
    "    # 단어 개수,(1,2)는 하나나 둘을 줄 수 있다는 것(전부를 포함)\n",
    "    , ngram_range=(1,2)\n",
    "    # 3개 미만은 제외시킴\n",
    "    , min_df=3\n",
    "    # 상위 10% 제외\n",
    "    , max_df=0.9\n",
    ")\n",
    "\n",
    "tfidf_vect.fit(x_train['document'])\n",
    "tfidf_train = tfidf_vect.transform(x_train['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차검증 및 하이퍼 파라미터 튜닝\n",
    "# Logistic Regression을 이용하여 감성 분석 Classification 수행\n",
    "# alpha 값의 역수(값이 작을수록 규제가 강한 것)\n",
    "# 로지스틱 회귀의 하이퍼 파라미터 C를 설정\n",
    "# C는 규제 강도를 조절하는  alpha 값의 역수로 작을수록 규제강도가 크며\n",
    "# Penalty는 규제의 유형을 설정하며 11 규제와 12규제가 있으며 기본은 12임\n",
    "lr_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "# Parameter 규제 강도를 나타내는 C 최적화\n",
    "params = {'C': [1, 3.5, 4.5, 5.5, 10]}\n",
    "\n",
    "gcv_lr = GridSearchCV(\n",
    "    lr_clf\n",
    "    , param_grid=params\n",
    "    , cv=3\n",
    "    , scoring='accuracy'\n",
    "    , verbose=0\n",
    ")\n",
    "\n",
    "#gcv_lr.fit(tfidf_matrix_train, x_train['label'])\n",
    "\n",
    "gcv_lr.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_lr.best_params_, round(gcv_lr.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 검증을 가지고 최종 검증\n",
    "# 테스트 세트를 이용 예측 시 학습할 때 적용한 TfidfVectorizer를 그대로 사용해야 함\n",
    "# 그래야 학습 시 설정한 피처 개수와 테스트 데이터를 TfidfVectorizer로 변경할 피처 개수가 같아짐\n",
    "from sklearn.metrics import accuracy_score\n",
    "tfidf_matrix_test = tfidf_vect.transform(x_test['document'])\n",
    "\n",
    "best_estimator = gcv_lr.best_estimator_\n",
    "lr_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('Logistic Regression 정확도: ', accuracy_score(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf를 이용하여 감성 분석 Classification 수행\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_params = {\n",
    "    # 나무 개수\n",
    "    'n_estimators':[50, 100, 200]\n",
    "    , 'max_depth':[2, 3, 5]\n",
    "    , 'min_samples_leaf':[1, 5, 8]}\n",
    "\n",
    "gcv_rf = GridSearchCV(rf_clf\n",
    "                      # 매개변수들\n",
    "                      , param_grid=rf_params\n",
    "                      # 판정하기 위한 기준\n",
    "                      , scoring = 'accuracy'\n",
    "                      # 교차 검증 횟수\n",
    "                      , cv=3\n",
    "                      , verbose=0)\n",
    "\n",
    "gcv_rf.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_rf.best_params_, round(gcv_rf.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 검증을 가지고 최종 검증\n",
    "# 테스트 세트를 이용 예측 시 학습할 때 적용한 TfidfVectorizer를 그대로 사용해야 함\n",
    "# 그래야 학습 시 설정한 피처 개수와 테스트 데이터를 TfidfVectorizer로 변경할 피처 개수가 같아짐\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_test = tfidf_vect.transform(x_test.document)\n",
    "best_estimator = gcv_rf.best_estimator_\n",
    "rf_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('RandomForest 정확도: ', accuracy_score(y_test, rf_preds))\n",
    "\n",
    "print(y_test.values[:10])\n",
    "print(rf_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt를 이용하여 감성 분석 Classification 수행\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth':[2,3,5,10]\n",
    "    , 'max_depth':[2, 3, 5]\n",
    "    , 'min_samples_leaf':[1, 5, 8]}\n",
    "\n",
    "gcv_dt = GridSearchCV(dt_clf\n",
    "                      # 매개변수들\n",
    "                      , param_grid=dt_params\n",
    "                      # 판정하기 위한 기준\n",
    "                      , scoring = 'accuracy'\n",
    "                      # 교차 검증 횟수\n",
    "                      , cv=3\n",
    "                      , verbose=0)\n",
    "\n",
    "gcv_dt.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_dt.best_params_, round(gcv_dt.best_score_, 4))\n",
    "\n",
    "tfidf_test = tfidf_vect.transform(x_test.document)\n",
    "best_estimator = gcv_dt.best_estimator_\n",
    "dt_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('Decision Tree 정확도: ', accuracy_score(y_test, dt_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
