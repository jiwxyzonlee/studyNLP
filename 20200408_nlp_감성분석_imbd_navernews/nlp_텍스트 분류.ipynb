{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 분석 수행 프로세스\n",
    "#### 20 뉴스그룹 데이터 세트를 이용, 텍스트 분류\n",
    "- 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측\n",
    "- 텍스트를 피처 벡터화로 변환, 희소 행렬로 만들고 로지스틱 회귀를 이용해 분류 수행\n",
    "- Count 기반 과 TF-IDF 기반의 벡터화를 각각 적용, 성능 비교\n",
    "- 피처 벡터화를 위한 파라미터와 GridSearchCV 기반의 하이퍼파라미터 튜닝을 일괄적으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정규화\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "len(news_data.data)\n",
    "pd.Series(news_data.target).value_counts()\n",
    "# print(news_data.target_names)\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습데이터 크기 11314, 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "source": [
    "# 순수한 텍스트만으로 구성된 기사 내용으로 어떤 뉴스그룹에 속하는지 분류\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# print(news_data.DESCR)\n",
    "# The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
    "# 20 topics split in two subsets: one for training (or development)\n",
    "# and the other one for testing (or for performance evaluation). The split\n",
    "# between the train and test set is based upon a messages posted before\n",
    "# and after a specific date.\n",
    "\n",
    "# 텍스트 정규화\n",
    "# 뉴스그룹 기사내용을 제외하고 다른 정보 제거\n",
    "# 제목, 소속, 이메일 등 헤더와 푸터 정보들은 분류의 타겟 클래스 값과 유사할 수 있음\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),\n",
    "                  random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "test_news = fetch_20newsgroups(subset='test',remove=('header','footers','quotes'),\n",
    "                              random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "print('학습데이터 크기 {0}, 테스트 데이터 크기 {1}'.format(len(train_news.data),\n",
    "                                           len(test_news.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631)\n",
      "(7532, 101631)\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train,y_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 반드시 이용하여 테스트 데이터 \n",
    "# feature extraction 변환 수행(피처 개수가 동일해야 함)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "print(X_train_cnt_vect.shape)\n",
    "print(X_test_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6481678173127987\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀를 적용, 뉴스그룹에 대한 분류 예측\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print(accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710169941582581\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF가 단순 카운트 기반 보다 높은 예측 정확도를 제공\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환.\n",
    "tfidf_vect =TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행.\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7250398300584174\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 분석에서 ML 성능 향상 : 최적의 알고리즘 선택, 최상의 피처 전처리\n",
    "# 텍스트 정규화, 피터 벡터화(TF-IDF 벡터화에 다양한 파라미터 적용)\n",
    "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경\n",
    "# max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
    "# n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2),max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Random Forest, SVM을 적용 뉴스그룹에 대한 분류 예측을 수행하세요.\n",
    "svm_clf = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756505576208178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train_tfidf_vect,y_train)\n",
    "rf_pred = rf_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692777482740308\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel='rbf',random_state=0)\n",
    "svm_clf.fit(X_train_tfidf_vect,y_train)\n",
    "svm_pred = svm_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "[12 12 12  7  8  1 16  6  4  8]\n",
      "[ 4 11  1  7  8  1 18  6  4  8]\n",
      "From: jkjec@westminster.ac.uk (Shazad Barlas)\n",
      "Subject: Re: MR2 - noisy engine.\n",
      "Organization: University of Westminster\n",
      "Lines: 10\n",
      "\n",
      "Just a quick note on the nwe shape MR2s in the UK.... \n",
      "\n",
      "When they first came out here, there were 3 models. The base model had an \n",
      "auto box and engine from the CAMRY 2.0 !!! Well I recentyl found out that this \n",
      "model is no longer profitable for Toyota and have since scraped it. I've also\n",
      "noticed that auto MR2s have depreciated a lot more than the next model up...\n"
     ]
    }
   ],
   "source": [
    "print(news_data.target_names)\n",
    "print(svm_pred[0:10])\n",
    "print(y_test[0:10])\n",
    "print(X_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train_tfidf_vect,y_train)\n",
    "dt_pred = dt_clf(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train_tfidf_vect,y_train)\n",
    "knn_pred = knn_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 28.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10}\n",
      "0.7287573021773766\n"
     ]
    }
   ],
   "source": [
    "# 30분 이상 걸림\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정.\n",
    "params = {'C':[5,10]}\n",
    "gcv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, \\\n",
    "                          scoring='accuracy',verbose = 1)\n",
    "gcv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print(gcv_lr.best_params_)\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가.\n",
    "lr_pred = gcv_lr.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test, lr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7287573021773766\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 사용 및 GridSearchCV와의 결합\n",
    "from sklearn.pipeline import Pipeline\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 \n",
    "# lr_clf 객체명으로 생성하는 Pipeline생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english',\\\n",
    "                                   ngram_range=(1,2),max_df=300)),\\\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), \n",
    "# predict( )가 필요 없음. \n",
    "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 \n",
    "# ML 학습/예측이 가능. \n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30분 이상 소요\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression())\n",
    "])\n",
    "# 하이퍼파라미터명이 개게 변수명과 결합 : 피처 벡터화 객체 파라미터와 Estimator 객체의 하이퍼파라미터 구별하기 위함\n",
    "params = {'tfidf_vect__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "         'tfidf_vect__max_df':[100,300,700],\n",
    "         'lr_clf__C':[1,5,10]}\n",
    "\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, \\\n",
    "                           scoring = 'accuracy', verbose = 1)\n",
    "grid_cv_pipe.fit(X_train,y_train)\n",
    "print(grid_cv_pipe.best_params_, grid_cv_pripe.best_score_)\n",
    "\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print(accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 뉴스기사의 카테고리 판정 - 네이버 뉴스 2400개 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., 0., 0.]), array([0.5       , 0.84657359, 0.        , 0.        , 0.        ]), array([0.25      , 0.4232868 , 0.59657359, 0.59657359, 0.        ]), array([0.5, 0. , 0. , 0. , 1. ])]\n",
      "{'_id': 5, '비': 0, '오늘': 1, '덥다': 2, '오후': 3, '일요일': 4}\n"
     ]
    }
   ],
   "source": [
    "# tfidf.py : TF-IDF로 텍스트를 벡터로 변환하는 모듈\n",
    "from konlpy.tag import Okt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# KoNLPy의 Okt객체 초기화 \n",
    "okt = Okt()\n",
    "# 전역 변수 \n",
    "word_dic = {'_id': 0} # 단어 사전\n",
    "dt_dic = {} # 문장 전체에서의 단어 출현 횟수\n",
    "files = [] # 문서들을 저장할 리스트\n",
    "\n",
    "def tokenize(text):\n",
    "    '''KoNLPy로 형태소 분석하기''' \n",
    "    result = []\n",
    "    word_s = okt.pos(text, norm=True, stem=True)\n",
    "    for n, h in word_s:\n",
    "        if not (h in ['Noun', 'Verb ', 'Adjective']): continue\n",
    "        if h == 'Punctuation' and h2 == 'Number': continue\n",
    "        result.append(n)\n",
    "    return result\n",
    "\n",
    "def words_to_ids(words, auto_add = True):\n",
    "    ''' 단어를 ID로 변환하기 ''' \n",
    "    result = []\n",
    "    for w in words:\n",
    "        if w in word_dic:\n",
    "            result.append(word_dic[w])\n",
    "            continue\n",
    "        elif auto_add:\n",
    "            id = word_dic[w] = word_dic['_id']\n",
    "            word_dic['_id'] += 1\n",
    "            result.append(id)\n",
    "    return result\n",
    "\n",
    "def add_text(text):\n",
    "    '''텍스트를 ID 리스트로 변환해서 추가하기''' \n",
    "    ids = words_to_ids(tokenize(text))\n",
    "    files.append(ids)\n",
    "\n",
    "def add_file(path):\n",
    "    '''텍스트 파일을 학습 전용으로 추가하기''' \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        s = f.read()\n",
    "        add_text(s)\n",
    "\n",
    "def calc_files():\n",
    "    '''추가한 파일 계산하기''' \n",
    "    global dt_dic\n",
    "    result = []\n",
    "    doc_count = len(files)\n",
    "    dt_dic = {}\n",
    "    # 단어 출현 횟수 세기\n",
    "    for words in files:\n",
    "        used_word = {}\n",
    "        data = np.zeros(word_dic['_id'])\n",
    "        for id in words:\n",
    "            data[id] += 1\n",
    "            used_word[id] = 1\n",
    "        # 단어 t가 사용되고 있을 경우 dt_dic의 수를 1 더하기 \n",
    "        for id in used_word:\n",
    "            if not(id in dt_dic): dt_dic[id] = 0\n",
    "            dt_dic[id] += 1\n",
    "        # 정규화하기 \n",
    "        data = data / len(words) \n",
    "        result.append(data)\n",
    "    # TF-IDF 계산하기 \n",
    "    for i, doc in enumerate(result):\n",
    "        for id, v in enumerate(doc):\n",
    "            idf = np.log(doc_count / dt_dic[id]) + 1\n",
    "            doc[id] = min([doc[id] * idf, 1.0])\n",
    "        result[i] = doc\n",
    "    return result\n",
    "\n",
    "def save_dic(fname):\n",
    "    '''사전을 파일로 저장하기''' \n",
    "    pickle.dump(\n",
    "        [word_dic, dt_dic, files],\n",
    "        open(fname, \"wb\"))\n",
    "\n",
    "def load_dic(fname):\n",
    "    '''사전 파일 읽어 들이기''' \n",
    "    global word_dic, dt_dic, files\n",
    "    n = pickle.load(open(fname, 'rb'))\n",
    "    word_dic, dt_dic, files = n\n",
    "\n",
    "def calc_text(text):\n",
    "    ''' 문장을 벡터로 변환하기 ''' \n",
    "    data = np.zeros(word_dic['_id'])\n",
    "    words = words_to_ids(tokenize(text), False)\n",
    "    for w in words:\n",
    "        data[w] += 1\n",
    "    data = data / len(words)\n",
    "    for id, v in enumerate(data):\n",
    "        idf = np.log(len(files) / dt_dic[id]) + 1\n",
    "        data[id] = min([data[id] * idf, 1.0])\n",
    "    return data\n",
    "# 모듈 테스트하기 \n",
    "if __name__ == '__main__':\n",
    "    add_text('비')\n",
    "    add_text('오늘은 비가 내렸어요.') \n",
    "    add_text('오늘은 더웠지만 오후부터 비가 내렸다.') \n",
    "    add_text('비가 내리는 일요일이다.') \n",
    "    print(calc_files())\n",
    "    print(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_files= dataset/100\n",
      "read_files= dataset/101\n",
      "read_files= dataset/103\n",
      "read_files= dataset/105\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 네이버 뉴스 정치, 경제, 생활, IT/과학 기사 TF-IDF 벡터 변환 후 genre.pickle로 저장\n",
    "# 텍스트 분류 과정\n",
    "# 1. 텍스트에서 불필요한 품사를 제거\n",
    "# 2. 사전을 기반으로 단어를 숫자로 변환\n",
    "# 3. 파일 내부의 단어 출현 비율을 계산\n",
    "# 4. 데이터를 학습\n",
    "\n",
    "# makedb_tfidf.py : 문장의 형태소를 벡터로 변환\n",
    "import os, glob, pickle\n",
    "import tfidf\n",
    "\n",
    "# 변수 초기화\n",
    "y = []\n",
    "x = []\n",
    "\n",
    "# 디렉터리 내부의 파일 목록 전체에 대해 처리하기 \n",
    "def read_files(path, label):\n",
    "    print(\"read_files=\", path)\n",
    "    files = glob.glob(path + \"/*.txt\")\n",
    "    for f in files:\n",
    "        if os.path.basename(f) == 'LICENSE.txt': continue\n",
    "        tfidf.add_file(f)\n",
    "        y.append(label)\n",
    "\n",
    "# 기사를 넣은 디렉터리 읽어 들이기 \n",
    "read_files('dataset/100', 0)\n",
    "read_files('dataset/101', 1)\n",
    "read_files('dataset/103', 2)\n",
    "read_files('dataset/105', 3)\n",
    "\n",
    "\n",
    "# TF-IDF 벡터로 변환하기 \n",
    "x = tfidf.calc_files()\n",
    "\n",
    "# 저장하기 \n",
    "pickle.dump([y, x], open('dataset/genre.pickle', 'wb'))\n",
    "tfidf.save_dic('dataset/genre-tdidf.dic')\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197\n",
      "3197\n",
      "정답률= 0.7828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       161\n",
      "           1       0.84      0.62      0.71       177\n",
      "           2       0.71      0.87      0.78       154\n",
      "           3       0.72      0.79      0.75       148\n",
      "\n",
      "    accuracy                           0.78       640\n",
      "   macro avg       0.79      0.79      0.78       640\n",
      "weighted avg       0.79      0.78      0.78       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "# TF-IDF 데이터베이스 읽어 들이기\n",
    "data = pickle.load(open(\"dataset/genre.pickle\", \"rb\"))\n",
    "y = data[0] # 레이블\n",
    "x = data[1] # TF-IDF\n",
    "\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "# 학습 전용과 테스트 전용으로 구분하기 \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2)\n",
    "\n",
    "# 나이브 베이즈로 학습하기 \n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# 평가하고 결과 출력하기 \n",
    "y_pred = model.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "rep = metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"정답률=\", acc)\n",
    "print(rep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. genre.pickle 이름으로 데이터를 불러와서 LR, RF, DT, SVM, KNN 모델로 학습 및 펑가를 수행하고 정확도 및 Classification_report를 출력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "# TF-IDF 데이터베이스 읽어 들이기\n",
    "data = pickle.load(open(\"dataset/genre.pickle\", \"rb\"))\n",
    "y = data[0] # 레이블\n",
    "x = data[1] # TF-IDF\n",
    "\n",
    "# 학습 전용과 테스트 전용으로 구분하기 \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "rep = metrics.classification_report(y_test, y_pred)\n",
    "print('정확도:', acc)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
