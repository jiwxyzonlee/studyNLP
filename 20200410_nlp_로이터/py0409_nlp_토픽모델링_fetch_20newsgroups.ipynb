{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링\n",
    "- 머신러닝 기반의 토픽 모델링을 적용해 문서 집합에 숨어 있는 주제를 찾아냄\n",
    "> 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것에 반해 머신러닝 기반의 토픽 모델링은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출\n",
    "- LSA(Latent Sementic Analysis) 와 LDA(Latent Dirichlet Allocation) 기법\n",
    " - LSA는 단어-문서행렬(Word-Document Matrix), 단어-문맥행렬(window based co-occurrence matrix) 등 입력 데이터에 특이값 분해를 수행해 데이터의 차원수를 줄여 계산 효율성을 키우면서 행간에 숨어있는(latent) 의미를 이끌어내기 위한 방법론\n",
    " - LDA는 미리 알고 있는 주제별 단어수 분포를 바탕으로, 주어진 문서에서 발견된 단어수 분포를 분석, 해당 문서가 어떤 주제들을 함께 다루고 있을지를 예측\n",
    " \n",
    " Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents. This is known as 'unsupervised' machine learning because it doesn't require a predefined list of tags or training data that's been previously classified by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7862, 1000)\n",
      "I appreciate if anyone can point out some good books about the dead sea\n",
      "scrolls of Qumran. Thanks in advance.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\"\"\"\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제를 추출\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics'\n",
    "        , 'comp.windows.x', 'talk.politics.guns'\n",
    "        , 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
    "\n",
    "# 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 categories에 cats 입력\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('header', 'footer', 'quotes'), categories=cats, random_state=0)\n",
    "\n",
    "# LDA 는 Count기반의 Vectorizer만 적용\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print(feat_vect.shape)\n",
    "\"\"\"\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제를 추출.\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', \\\n",
    "        'comp.windows.x', 'talk.politics.mideast', 'soc.religion.christian',\\\n",
    "        'sci.electronics', 'sci.med'  ]\n",
    "# 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 \n",
    "# categories에 cats 입력\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            categories = cats, random_state=0)\n",
    "# LDA 는 Count기반의 Vectorizer만 적용\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2,\\\n",
    "                            stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print(feat_vect.shape)\n",
    "print(news_df.data[0])\n",
    "#print(feat_vect[0].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_vect.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=8, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/decomposition.html#latentdirichletallocation\n",
    "\n",
    "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\n",
    "\n",
    "The graphical model of LDA is a three-level generative mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screenshot](\"./images/lda_model_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013):\n",
    "\n",
    "The corpus is a collection of  documents.\n",
    "\n",
    "A document is a sequence of  words.\n",
    "\n",
    "There are  topics in the corpus.\n",
    "\n",
    "The boxes represent repeated sampling.\n",
    "\n",
    "In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14, 13, 12, 11, 10,  9,  8,  7,  6,  5], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argsort() 넘파이 배열의 원소를 오름차순으로 정렬하는 메소드임\n",
    "import numpy as np\n",
    "d1 = np.arange(10, 25)\n",
    "print(d1)\n",
    "\n",
    "d2 = d1.argsort() # 오름차순\n",
    "print(d2)\n",
    "\n",
    "topic_word_indexes = d1.argsort()[::-1] # 내림차순\n",
    "print(topic_word_indexes)\n",
    "\n",
    "top_indexes = topic_word_indexes[:10]\n",
    "top_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
      "Topic # 1\n",
      "don just like know people said think time ve didn right going say ll way\n",
      "Topic # 2\n",
      "image file jpeg program gif images output format files color entry 00 use bit 03\n",
      "Topic # 3\n",
      "like know don think use does just good time book read information people used post\n",
      "Topic # 4\n",
      "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
      "Topic # 5\n",
      "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
      "Topic # 6\n",
      "god people jesus church believe christ does christian say think christians bible faith sin life\n",
      "Topic # 7\n",
      "use dos thanks windows using window does display help like problem server need know run\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model,feature_names,no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes]) # feature_names를 공백으로 join\n",
    "        print(feature_concat)\n",
    "\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출        \n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)\n",
    "\n",
    "# cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', \\\n",
    "#         'comp.windows.x', 'talk.politics.mideast', 'soc.religion.christian',\\\n",
    "#         'sci.electronics', 'sci.med'  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news20_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'))\n",
    "print(news20_df.keys())\n",
    "news20_df.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied from: \n",
    "https://scikit-learn.org/stable/datasets/index.html#newsgroups-dataset\n",
    "\n",
    "### The 20 newsgroups text dataset\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "This module contains two loaders. The first one, sklearn.datasets.fetch_20newsgroups, returns a list of the raw texts that can be fed to text feature extractors such as sklearn.feature_extraction.text.CountVectorizer with custom parameters so as to extract feature vectors. The second one, sklearn.datasets.fetch_20newsgroups_vectorized, returns ready-to-use features, i.e., it is not necessary to use a feature extractor.\n",
    "\n",
    "#### Usage\n",
    "\n",
    "The sklearn.datasets.fetch_20newsgroups function is a data fetching / caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~/scikit_learn_data/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them:\n",
    "\n",
    ">from sklearn.datasets import fetch_20newsgroups\n",
    ">\n",
    "> newsgroups_train = fetch_20newsgroups(subset='train')\n",
    ">\n",
    "> from pprint import pprint\n",
    ">\n",
    "> pprint(list(newsgroups_train.target_names))\n",
    "\n",
    "['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (18846, 1000)\n",
      "\n",
      "FOR SALE\n",
      "\n",
      "                 1945 King Feature Syndicate\n",
      "                 Jaymar Specialty Company\n",
      "                 200 Fifth Avenue New York, NY\n",
      "\n",
      "                 Cardboard puzzle - NO BOX\n",
      "                 Pieces worn from use\n",
      "                 NO MISSING PIECES\n",
      "                 Size: 13 3/4 inches by 21 1/2 inches\n",
      "                 60 Puzzle Pieces\n",
      "\n",
      "   Puzzle depicts Dagwood, Blondie, the kids, and dog Daisey with her\n",
      "   puppies on a picnic with Dagwood and Alexander trying to get\n",
      "   a fishing line out of a tree.\n"
     ]
    }
   ],
   "source": [
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 8개 주제를 추출.\n",
    "dogs = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "news_df0 = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes')\n",
    "                             , categories = dogs, random_state=0)\n",
    "# LDA 는 Count기반의 Vectorizer만 적용\n",
    "count_vect = CountVectorizer(\n",
    "    max_df=0.95\n",
    "    , max_features=1000\n",
    "    , min_df=2\n",
    "    , stop_words='english'\n",
    "    , ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "feat_vect = count_vect.fit_transform(news_df0.data)\n",
    "print('shape: ', feat_vect.shape); print()\n",
    "print(news_df0.data[0])\n",
    "#print(feat_vect[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=12, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=12, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.39469574e-01, 8.33348059e-02, 8.33358581e-02, ...,\n",
       "        8.33362650e-02, 8.33352742e-02, 8.33351868e-02],\n",
       "       [2.88621344e-01, 2.44540281e+00, 1.91765266e-01, ...,\n",
       "        5.06485320e+01, 8.07463074e-01, 2.50887610e+00],\n",
       "       [8.33343439e-02, 7.99417886e+00, 2.59375616e+00, ...,\n",
       "        1.51646327e+02, 2.68679498e+00, 1.93496691e+02],\n",
       "       ...,\n",
       "       [1.73463667e-01, 1.16912548e+02, 9.80581779e+00, ...,\n",
       "        1.73620118e+02, 8.33343608e-02, 8.33349021e-02],\n",
       "       [4.79149104e+01, 1.05611128e+02, 3.95812615e+00, ...,\n",
       "        8.33365761e-02, 1.20410251e+02, 1.82289497e+01],\n",
       "       [8.33345558e-02, 5.11426974e+02, 8.47992925e-02, ...,\n",
       "        1.05589750e+02, 1.87311405e+02, 9.08093391e+01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14, 13, 12, 11, 10,  9,  8,  7,  6,  5], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argsort() 넘파이 배열의 원소를 오름차순으로 정렬하는 메소드임\n",
    "import numpy as np\n",
    "d1 = np.arange(10, 25)\n",
    "print(d1)\n",
    "\n",
    "d2 = d1.argsort() # 오름차순\n",
    "print(d2)\n",
    "\n",
    "topic_word_indexes = d1.argsort()[::-1] # 내림차순\n",
    "print(topic_word_indexes)\n",
    "\n",
    "top_indexes = topic_word_indexes[:10]\n",
    "top_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "edu image file color jpeg images format files gif version quality 24 display convert programs\n",
      "Topic # 1\n",
      "key data available edu software graphics use ftp com chip pub mail information encryption bit\n",
      "Topic # 2\n",
      "time just said didn year don did like think game know got years went going\n",
      "Topic # 3\n",
      "ax ax ax max max ax ax max g9v b8f a86 g9v g9v pl 145 1d9 a86 a86 b8f b8f 34u\n",
      "Topic # 4\n",
      "10 00 25 12 11 15 20 16 14 17 13 50 18 19 30\n",
      "Topic # 5\n",
      "windows file use program dos window using problem thanks run does set com screen application\n",
      "Topic # 6\n",
      "god people don think just does like say believe know way good jesus make point\n",
      "Topic # 7\n",
      "know don thanks does mr think president db like just don know going ve mail ll\n",
      "Topic # 8\n",
      "games new team game sale hockey price players best offer good edu league list shipping\n",
      "Topic # 9\n",
      "drive like just use car used problem scsi ve power good don hard work card\n",
      "Topic # 10\n",
      "space university research information nasa new center 1993 program earth dos national science dos dos launch\n",
      "Topic # 11\n",
      "people government armenian gun law state israel jews war armenians right rights states turkish children\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model,feature_names,no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes]) # feature_names를 공백으로 join\n",
    "        print(feature_concat)\n",
    "\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출        \n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. fetch_20newsgroups로 다음 작업을 수행\n",
    "- TfidfVectorizer 방식으로 벡터 처리, lr 알고리즘으로 precision 포함하여 평가\n",
    "- precision 기준으로 평가지수 높은 순으로 5개 그룹을 선정하여 토픽 모델링 수행\n",
    "- 텍스트 분류 예측 정밀도와 그룹별로 토픽 모델링 성능간의 상관관계 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  (18846, 8930)\n",
      "y_train: (18846,)\n",
      "x_test (18846, 8930)\n",
      "y_test (18846,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from numpy import r_\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train'\n",
    "    , remove=('headers','footers','quotes')\n",
    "    , random_state=156\n",
    ")\n",
    "\n",
    "newsgroups_test = fetch_20newsgroups(\n",
    "    subset='test'\n",
    "    , remove=('headers','footers','quotes')\n",
    "    , random_state=156\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.001, max_df=0.20)\n",
    "tfidf_vect = tfidf.fit_transform(newsgroups_train.data)\n",
    "tfidf_vect_test = tfidf.fit_transform(newsgroups_test.data)\n",
    "\n",
    "x_train = tfidf_vect\n",
    "print('x_train: ', x_train.shape)\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "print('y_train:', y_train.shape)\n",
    "x_test = tfidf_vect_test\n",
    "print('x_test', x_test.shape)\n",
    "y_test = newsgroups_test.target\n",
    "print('y_test', y_test.shape)\n",
    "\n",
    "# x = np.array(np.r_[x_train.todense(), x_test.todense()])\n",
    "# y = np.r_[y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 0.8647458346598748\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.86      0.81      0.83       799\n",
      "           comp.graphics       0.85      0.83      0.84       973\n",
      " comp.os.ms-windows.misc       0.83      0.81      0.82       985\n",
      "comp.sys.ibm.pc.hardware       0.84      0.83      0.84       982\n",
      "   comp.sys.mac.hardware       0.90      0.85      0.87       963\n",
      "          comp.windows.x       0.91      0.88      0.89       988\n",
      "            misc.forsale       0.87      0.86      0.87       975\n",
      "               rec.autos       0.89      0.85      0.87       990\n",
      "         rec.motorcycles       0.59      0.92      0.72       996\n",
      "      rec.sport.baseball       0.95      0.91      0.93       994\n",
      "        rec.sport.hockey       0.98      0.93      0.95       999\n",
      "               sci.crypt       0.95      0.87      0.91       991\n",
      "         sci.electronics       0.85      0.85      0.85       984\n",
      "                 sci.med       0.92      0.92      0.92       990\n",
      "               sci.space       0.92      0.91      0.91       987\n",
      "  soc.religion.christian       0.84      0.91      0.88       997\n",
      "      talk.politics.guns       0.84      0.89      0.87       910\n",
      "   talk.politics.mideast       0.95      0.92      0.94       940\n",
      "      talk.politics.misc       0.88      0.82      0.85       775\n",
      "      talk.religion.misc       0.90      0.60      0.72       628\n",
      "\n",
      "                accuracy                           0.86     18846\n",
      "               macro avg       0.88      0.86      0.86     18846\n",
      "            weighted avg       0.87      0.86      0.87     18846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vect - Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TF-IDF Vect - Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "lr_pred = lr.predict(x_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print('accuracy=', lr_accuracy)\n",
    "print()\n",
    "\n",
    "from sklearn import metrics\n",
    "report = metrics.classification_report(y_test, lr_pred, target_names = categories)\n",
    "print(report)\n",
    "\n",
    "\n",
    "# 'rec.sport.baseball',\n",
    "# 'rec.sport.hockey',\n",
    "# 'sci.crypt',\n",
    "# 'sci.med',\n",
    "# 'talk.politics.mideast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: atheist atheists atheism god religion\n",
      "comp.graphics: pov images image 3d graphics\n",
      "comp.os.ms-windows.misc: ax file cica microsoft windows\n",
      "comp.sys.ibm.pc.hardware: ide drive card bios pc\n",
      "comp.sys.mac.hardware: se centris lc apple mac\n",
      "comp.windows.x: widget xterm window server motif\n",
      "misc.forsale: asking sell shipping offer sale\n",
      "rec.autos: oil engine ford cars car\n",
      "rec.motorcycles: motorcycle bikes ride dod bike\n",
      "rec.sport.baseball: ball year team game baseball\n",
      "rec.sport.hockey: season nhl team game hockey\n",
      "sci.crypt: government key nsa encryption clipper\n",
      "sci.electronics: motorola voltage battery electronics circuit\n",
      "sci.med: photography disease msg medical doctor\n",
      "sci.space: sky launch shuttle orbit space\n",
      "soc.religion.christian: christianity christians christian church god\n",
      "talk.politics.guns: firearms fbi weapons guns gun\n",
      "talk.politics.mideast: armenians jews arab israeli israel\n",
      "talk.politics.misc: gay people clayton clinton government\n",
      "talk.religion.misc: koresh jesus christian kent god\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.asarray(tfidf.get_feature_names())\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "        top5 = np.argsort(lr.coef_[i])[-5:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4914, 1000)\n",
      "Elias' initial statement certain *is* hot air. But it seems to be\n",
      "almost standard procedure around here to first throw out an absurb,\n",
      "overstated image in order to add extra \"meaning\" to the posting's\n",
      "*real point*. \n",
      "\n",
      "However, his second statement *is* quite real. The essential sealing off\n",
      "of Gaza residents from the possibility of making a living *has happened*.\n",
      "Certainly, the Israeli had a legitimate worry behind the action they took,\n",
      "but isn't that action a little draconian?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "models = ['rec.sport.baseball',\n",
    "'rec.sport.hockey',\n",
    "'sci.crypt',\n",
    "'sci.med',\n",
    "'talk.politics.mideast'  ]\n",
    "# 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 \n",
    "# categories에 cats 입력\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            categories = models, random_state=0)\n",
    "# LDA 는 Count기반의 Vectorizer만 적용\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2,\\\n",
    "                            stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print(feat_vect.shape)\n",
    "print(news_df.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.09476082e+00, 7.61316452e+00, 3.01051510e-01, ...,\n",
       "        1.50868478e+00, 2.03580516e+01, 2.03428191e-01],\n",
       "       [6.65688807e+01, 2.43798745e+01, 1.14414332e+02, ...,\n",
       "        9.28108137e+01, 6.52216404e+01, 8.14314221e+01],\n",
       "       [2.01543778e-01, 3.05785593e+02, 2.00676470e-01, ...,\n",
       "        9.49873251e+01, 8.33073840e+01, 1.35670831e+01],\n",
       "       [5.13728599e-01, 8.68011505e+00, 2.01243905e-01, ...,\n",
       "        4.44716171e+00, 1.90355743e+00, 2.65921787e+01],\n",
       "       [3.39621086e+02, 1.73541253e+02, 5.18826963e+01, ...,\n",
       "        1.26246015e+02, 5.12093665e+01, 2.05887944e-01]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "[14 13 12 11 10  9  8  7  6  5  4  3  2  1  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([14, 13, 12, 11, 10,  9,  8,  7,  6,  5], dtype=int64)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # argsort() 넘파이 배열의 원소를 오름차순으로 정렬하는 메소드임\n",
    "# import numpy as np\n",
    "# d1 = np.arange(10, 25)\n",
    "# print(d1)\n",
    "\n",
    "# d2 = d1.argsort() # 오름차순\n",
    "# print(d2)\n",
    "\n",
    "# topic_word_indexes = d1.argsort()[::-1] # 내림차순\n",
    "# print(topic_word_indexes)\n",
    "\n",
    "# top_indexes = topic_word_indexes[:10]\n",
    "# top_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "don like edu know people just use time think good key does com used information\n",
      "Topic # 1\n",
      "game team year games play season hockey think players don good time just like league\n",
      "Topic # 2\n",
      "people armenian said armenians turkish did jews know turkey like just don children armenia killed\n",
      "Topic # 3\n",
      "israel government key chip encryption clipper law israeli people arab security keys use right state\n",
      "Topic # 4\n",
      "10 25 11 12 20 15 16 14 17 13 18 30 55 19 92\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model,feature_names,no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes]) # feature_names를 공백으로 join\n",
    "        print(feature_concat)\n",
    "\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출        \n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)\n",
    "\n",
    "\"\"\"\n",
    "models = ['rec.sport.baseball',\n",
    "'rec.sport.hockey',\n",
    "'sci.crypt',\n",
    "'sci.med',\n",
    "'talk.politics.mideast'  ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(news_data.target_names)\n",
    "# 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'talk.politics.mideast'\n",
    "# print(news_data.target[10])\n",
    "# print(news_data.data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),\n",
    "                  random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "test_news = fetch_20newsgroups(subset='test',remove=('header','footers','quotes'),\n",
    "                              random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "\n",
    "# 내 거는 튜플로 안 했으면 됐었을텐데 왜 튜플로 해가지고는.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect =TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710169941582581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.47      0.54       319\n",
      "           1       0.57      0.76      0.65       389\n",
      "           2       0.67      0.71      0.69       394\n",
      "           3       0.73      0.61      0.66       392\n",
      "           4       0.84      0.64      0.73       385\n",
      "           5       0.73      0.70      0.71       395\n",
      "           6       0.58      0.87      0.70       390\n",
      "           7       0.90      0.66      0.76       396\n",
      "           8       0.77      0.81      0.79       398\n",
      "           9       0.87      0.79      0.83       397\n",
      "          10       0.89      0.92      0.90       399\n",
      "          11       0.86      0.79      0.82       396\n",
      "          12       0.44      0.70      0.54       393\n",
      "          13       0.81      0.72      0.76       396\n",
      "          14       0.68      0.86      0.76       394\n",
      "          15       0.71      0.74      0.72       398\n",
      "          16       0.62      0.77      0.69       364\n",
      "          17       0.89      0.75      0.81       376\n",
      "          18       0.72      0.43      0.54       310\n",
      "          19       0.64      0.22      0.33       251\n",
      "\n",
      "    accuracy                           0.71      7532\n",
      "   macro avg       0.73      0.70      0.70      7532\n",
      "weighted avg       0.73      0.71      0.71      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "lr_pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print(accuracy_score(y_test, lr_pred))\n",
    "rp = metrics.classification_report(y_test,lr_pred)\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            random_state=0)\n",
    "news_df.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4937, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 풀이\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# cats = ['comp.sys.mac.hardware',\n",
    "#         'comp.windows.x',\n",
    "#         'rec.sport.baseball',\n",
    "#         'rec.sport.hockey',\n",
    "#         'misc.forsale']\n",
    "cats = [ 'rec.autos','rec.sport.baseball','rec.sport.hockey','sci.crypt','comp.sys.mac.hardware']\n",
    "news_df1 = fetch_20newsgroups(subset='all', remove=('headers','footers','quotes'),\n",
    "                            categories = cats, random_state=0)\n",
    "# LDA 는 Count기반의 Vectorizer만 적용\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2,\\\n",
    "                            stop_words='english', ngram_range=(1,2))\n",
    "feat_vect1 = count_vect.fit_transform(news_df1.data)\n",
    "print(feat_vect1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda1 = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda1.fit(feat_vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.81221751e-01, 8.17236876e+00, 1.13296386e+00, ...,\n",
       "        2.01107811e-01, 2.02368776e-01, 3.36927648e+00],\n",
       "       [2.32343648e-01, 3.91708640e+00, 2.22138431e-01, ...,\n",
       "        6.54346854e+00, 2.11812125e-01, 2.36478489e-01],\n",
       "       [1.48399734e+01, 1.13958651e+02, 2.09817782e+00, ...,\n",
       "        7.39111572e+00, 5.33963134e+01, 7.13572392e+01],\n",
       "       [4.09230018e+01, 5.98206306e+00, 1.25266756e+02, ...,\n",
       "        1.24770028e+02, 1.81816786e+01, 2.08166468e-01],\n",
       "       [3.47023459e+02, 9.09698310e+01, 3.92799641e+01, ...,\n",
       "        1.09427979e+00, 3.10078271e+01, 1.68288393e+01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda1.components_.shape)\n",
    "lda1.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "chip mac use apple bit know does drive like just problem new used thanks need\n",
      "Topic # 1\n",
      "key government encryption use people keys security public privacy information law message mail des edu\n",
      "Topic # 2\n",
      "don just like think car good year time game know db better team did right\n",
      "Topic # 3\n",
      "game hockey team games play period season nhl new gm st vs 03 02 chicago\n",
      "Topic # 4\n",
      "25 10 11 12 16 14 15 55 20 13 18 17 00 19 30\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model,feature_names,no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출        \n",
    "feature_names = count_vect.get_feature_names()\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda1, feature_names, 15)\n",
    "\n",
    "# cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', \\\n",
    "#         'comp.windows.x', 'talk.politics.mideast', 'soc.religion.christian',\\\n",
    "#         'sci.electronics', 'sci.med'  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [ 'rec.autos','rec.sport.baseball','rec.sport.hockey','sci.crypt','comp.sys.mac.hardware']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
