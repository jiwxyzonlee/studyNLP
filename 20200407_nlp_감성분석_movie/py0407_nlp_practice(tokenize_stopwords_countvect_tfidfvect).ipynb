{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 뉴스 문서에 대해 모든 단어를 토큰화 english stop word 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내 풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A', 'vacuum', 'of', 'knowledge', 'about', 'the', 'origins', 'of', 'the', 'new', 'coronavirus', 'ravaging', 'the', 'world', 'has', 'provided', 'fertile', 'ground', 'for', 'all', 'manner', 'of', 'theories', '--', 'from', 'the', 'fantastic', ',', 'to', 'the', 'dubious', 'to', 'the', 'believable', '.'], ['It', 'was', 'a', 'bioweapon', 'manufactured', 'by', 'the', 'Chinese', '.'], ['The', 'US', 'Army', 'brought', 'the', 'virus', 'to', 'Wuhan', '.'], ['It', 'leaked', '--', 'like', 'a', 'genie', 'out', 'of', 'a', 'bottle', '--', 'from', 'a', 'lab', 'in', 'an', 'accident', '.'], ['It', 'took', 'root', 'at', 'a', 'wildlife', 'market', 'in', 'Wuhan', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "corpus = '''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "'''\n",
    "sentences = sent_tokenize(text=corpus)\n",
    "#print(sentences)\n",
    "\n",
    "words = [word_tokenize(sentence) for sentence in sentences]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vacuum', 'knowledge', 'origins', 'new', 'coronavirus', 'ravaging', 'world', 'provided', 'fertile', 'ground', 'manner', 'theories', '--', 'fantastic', ',', 'dubious', 'believable', '.', 'bioweapon', 'manufactured', 'chinese', '.', 'us', 'army', 'brought', 'virus', 'wuhan', '.', 'leaked', '--', 'like', 'genie', 'bottle', '--', 'lab', 'accident', '.', 'took', 'root', 'wildlife', 'market', 'wuhan', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_w = nltk.corpus.stopwords.words('english')\n",
    "#print(stop_w)\n",
    "filtered_tokens = []\n",
    "\n",
    "for word in words:\n",
    "    for word_token in word:\n",
    "        word_token = word_token.lower()\n",
    "        if word_token not in stop_w:\n",
    "            filtered_tokens.append(word_token)\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['vacuum', 'knowledge', 'origins', 'new', 'coronavirus', 'ravaging', 'world', 'provided', 'fertile', 'ground', 'manner', 'theories', '--', 'fantastic', ',', 'dubious', 'believable', '.'], ['bioweapon', 'manufactured', 'chinese', '.'], ['us', 'army', 'brought', 'virus', 'wuhan', '.'], ['leaked', '--', 'like', 'genie', 'bottle', '--', 'lab', 'accident', '.'], ['took', 'root', 'wildlife', 'market', 'wuhan', '.']]\n",
      "\n",
      "type:  <class 'list'> , len:  5\n"
     ]
    }
   ],
   "source": [
    "# 정답\n",
    "\n",
    "import nltk\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # 문장별 분리 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "corpus = '''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "'''\n",
    "\n",
    "word_tokens = tokenize_text(corpus)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens); print()\n",
    "print('type: ', type(all_tokens), ', len: ', len(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동일한 문서에 대한 벡터화(ContVectorizer) english stop word 제거 -> 사전\n",
    "### 사전을 이용 'This is the second document' 벡터 표현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내 풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'accident', 'all', 'an', 'army', 'at', 'believable', 'bioweapon', 'bottle', 'brought', 'by', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'for', 'from', 'genie', 'ground', 'has', 'in', 'it', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'of', 'origins', 'out', 'provided', 'ravaging', 'root', 'the', 'theories', 'to', 'took', 'us', 'vacuum', 'virus', 'was', 'wildlife', 'world', 'wuhan']\n",
      "\n",
      "{'vacuum': 42, 'of': 31, 'knowledge': 23, 'about': 0, 'the': 37, 'origins': 32, 'new': 30, 'coronavirus': 12, 'ravaging': 35, 'world': 46, 'has': 20, 'provided': 34, 'fertile': 15, 'ground': 19, 'for': 16, 'all': 2, 'manner': 27, 'theories': 38, 'from': 17, 'fantastic': 14, 'to': 39, 'dubious': 13, 'believable': 6, 'it': 22, 'was': 44, 'bioweapon': 7, 'manufactured': 28, 'by': 10, 'chinese': 11, 'us': 41, 'army': 4, 'brought': 9, 'virus': 43, 'wuhan': 47, 'leaked': 25, 'like': 26, 'genie': 18, 'out': 33, 'bottle': 8, 'lab': 24, 'in': 21, 'an': 3, 'accident': 1, 'took': 40, 'root': 36, 'at': 5, 'wildlife': 45, 'market': 29}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "li = []\n",
    "corpus = '''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "'''\n",
    "li.append(corpus)\n",
    "\n",
    "countvect = CountVectorizer()\n",
    "countvect.fit(li)\n",
    "\n",
    "print(countvect.get_feature_names())\n",
    "print()\n",
    "print(countvect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
      "\n",
      "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer(stop_words='english').fit(li)\n",
    "countvect.vocabulary_\n",
    "\n",
    "print(countvect.get_feature_names())\n",
    "print()\n",
    "print(countvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
      "\n",
      "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "''']\n",
    "\n",
    "countvect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "\n",
    "print(countvect.get_feature_names())\n",
    "print()\n",
    "print(countvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "print(countvect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(countvect.transform(['This is the second documnet']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidVectorizer를 이용하여 동일 작업 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내 풀이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
      "\n",
      "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "li = []\n",
    "corpus = '''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "'''\n",
    "li.append(corpus)\n",
    "\n",
    "tfidfv = TfidfVectorizer()\n",
    "tfidfv = TfidfVectorizer(stop_words = 'english').fit(li)\n",
    "\n",
    "print(tfidfv.get_feature_names())\n",
    "print()\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 해답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accident', 'army', 'believable', 'bioweapon', 'bottle', 'brought', 'chinese', 'coronavirus', 'dubious', 'fantastic', 'fertile', 'genie', 'ground', 'knowledge', 'lab', 'leaked', 'like', 'manner', 'manufactured', 'market', 'new', 'origins', 'provided', 'ravaging', 'root', 'theories', 'took', 'vacuum', 'virus', 'wildlife', 'world', 'wuhan']\n",
      "\n",
      "{'vacuum': 27, 'knowledge': 13, 'origins': 21, 'new': 20, 'coronavirus': 7, 'ravaging': 23, 'world': 30, 'provided': 22, 'fertile': 10, 'ground': 12, 'manner': 17, 'theories': 25, 'fantastic': 9, 'dubious': 8, 'believable': 2, 'bioweapon': 3, 'manufactured': 18, 'chinese': 6, 'army': 1, 'brought': 5, 'virus': 28, 'wuhan': 31, 'leaked': 15, 'like': 16, 'genie': 11, 'bottle': 4, 'lab': 14, 'accident': 0, 'took': 26, 'root': 24, 'wildlife': 29, 'market': 19}\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['''\n",
    "A vacuum of knowledge about the origins of the new coronavirus ravaging the world has provided fertile ground for all manner of theories -- from the fantastic, to the dubious to the believable.\n",
    "It was a bioweapon manufactured by the Chinese. The US Army brought the virus to Wuhan. It leaked -- like a genie out of a bottle -- from a lab in an accident. It took root at a wildlife market in Wuhan.\n",
    "''']\n",
    "\n",
    "tfidfv = TfidfVectorizer(stop_words = 'english').fit(corpus)\n",
    "\n",
    "print(tfidfv.get_feature_names())\n",
    "print()\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16903085 0.16903085 0.16903085 0.16903085 0.16903085 0.16903085\n",
      "  0.16903085 0.16903085 0.16903085 0.16903085 0.16903085 0.16903085\n",
      "  0.16903085 0.16903085 0.16903085 0.16903085 0.16903085 0.16903085\n",
      "  0.16903085 0.16903085 0.16903085 0.16903085 0.16903085 0.16903085\n",
      "  0.16903085 0.16903085 0.16903085 0.16903085 0.16903085 0.16903085\n",
      "  0.16903085 0.3380617 ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidfv.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidfv.transform(['This is the second documnet']).toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
