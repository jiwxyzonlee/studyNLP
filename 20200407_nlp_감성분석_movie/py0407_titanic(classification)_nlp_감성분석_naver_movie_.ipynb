{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타이타닉 생존율 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_python\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_python\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_python\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pclass    1309 non-null   int64  \n",
      " 1   survived  1309 non-null   int64  \n",
      " 2   sex       1309 non-null   int32  \n",
      " 3   age       1309 non-null   float64\n",
      " 4   sibsp     1309 non-null   int64  \n",
      " 5   parch     1309 non-null   int64  \n",
      " 6   fare      1309 non-null   float64\n",
      " 7   cabin     1309 non-null   int32  \n",
      " 8   embarked  1309 non-null   int32  \n",
      "dtypes: float64(2), int32(3), int64(4)\n",
      "memory usage: 76.8 KB\n"
     ]
    }
   ],
   "source": [
    "t_df = pd.read_pickle('./dataset/t_df.pkl')\n",
    "t_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9751671442215855\n",
      "\n",
      "예측 정확도 0.7595419847328244\n"
     ]
    }
   ],
   "source": [
    "y_df = t_df.survived\n",
    "x_df = t_df.drop('survived', axis = 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df\n",
    "                                                    , y_df\n",
    "                                                    , test_size=0.2\n",
    "                                                    , random_state=0)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "dt_clf.fit(x_train, y_train)\n",
    "dt_pred = dt_clf.predict(x_test)\n",
    "\n",
    "print('Score: {}'.format(dt_clf.score(x_train, y_train)))\n",
    "print()\n",
    "\n",
    "df_accuracy = accuracy_score(y_test, dt_pred)\n",
    "print('예측 정확도', df_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 예측 정확도:  0.7938931297709924\n",
      "lr_clf.score(x_test, y_test)): 0.7938931297709924\n",
      "\n",
      "[train_score]:  0.789875835721108\n",
      "[test_score]:  0.7938931297709924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "lr_clf.fit(x_train, y_train)\n",
    "\n",
    "lr_pred = lr_clf.predict(x_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "print('lr 예측 정확도: ', lr_accuracy)\n",
    "\n",
    "# 아래와 같은 값임\n",
    "print('lr_clf.score(x_test, y_test)): ', lr_clf.score(x_test, y_test)); print()\n",
    "\n",
    "print('[train_score]: ', lr_clf.score(x_train, y_train))\n",
    "print('[test_score]: ', lr_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 사용자 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차 행렬\n",
      "[[135  27]\n",
      " [ 36  64]]\n",
      "\n",
      "정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n오차 행렬 (생존자 예측이므로 1을 기준으로 하기 때문에 TP와 TN이 뒤집어짐)\\n예측---N---------P\\n실N[[135(TN)  27(FP)]\\n제P [ 36(FN)  64(TP)]]\\n\\n정밀도 = TP / (TP + FP)\\n재현율 = TP / (TP + FN)\\nf1 = 2 * {정밀도 * 재현율 / (정밀도 + 재현율)}\\n\\n정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 사용자 함수\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    print()\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))\n",
    "\n",
    "get_clf_eval(y_test, dt_pred)\n",
    "\n",
    "\"\"\"\n",
    "오차 행렬 (생존자 예측이므로 1을 기준으로 하기 때문에 TP와 TN이 뒤집어짐)\n",
    "예측---N---------P\n",
    "실N[[135(TN)  27(FP)]\n",
    "제P [ 36(FN)  64(TP)]]\n",
    "\n",
    "정밀도 = TP / (TP + FP)\n",
    "재현율 = TP / (TP + FN)\n",
    "f1 = 2 * {정밀도 * 재현율 / (정밀도 + 재현율)}\n",
    "\n",
    "정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 교차 검증 - KFold\n",
    "- 문제가 많아서 제한적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screenshot](./images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using  of the folds as training data;\n",
    "\n",
    "the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screenshot](./images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증 0, 정확도: 0.5802\n",
      "교차 검증 1, 정확도: 0.7863\n",
      "교차 검증 2, 정확도: 0.8092\n",
      "교차 검증 3, 정확도: 0.7710\n",
      "교차 검증 4, 정확도: 0.7252\n",
      "교차 검증 5, 정확도: 0.7176\n",
      "교차 검증 6, 정확도: 0.6641\n",
      "교차 검증 7, 정확도: 0.6031\n",
      "교차 검증 8, 정확도: 0.6947\n",
      "교차 검증 9, 정확도: 0.7231\n",
      "\n",
      "평균 정확도: 0.7074\n"
     ]
    }
   ],
   "source": [
    "# cross_val_scores\n",
    "# KFold의 일련 과정을 한꺼번에 수행해주는 API\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(dt_clf, x_df, y_df, cv=10)\n",
    "for iter_count, accuracy in enumerate(scores):\n",
    "    print('교차 검증 {0}, 정확도: {1:.4f}'.format(iter_count, accuracy))\n",
    "\n",
    "print()\n",
    "print('평균 정확도: {0:.4f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "- CV 종합편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score=nan,\n",
      "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
      "                                              criterion='gini', max_depth=None,\n",
      "                                              max_features=None,\n",
      "                                              max_leaf_nodes=None,\n",
      "                                              min_impurity_decrease=0.0,\n",
      "                                              min_impurity_split=None,\n",
      "                                              min_samples_leaf=1,\n",
      "                                              min_samples_split=2,\n",
      "                                              min_weight_fraction_leaf=0.0,\n",
      "                                              presort='deprecated',\n",
      "                                              random_state=0, splitter='best'),\n",
      "             iid='deprecated', n_jobs=None,\n",
      "             param_grid={'max_depth': [2, 3, 5, 10],\n",
      "                         'min_samples_leaf': [1, 5, 8],\n",
      "                         'min_samples_split': [2, 3, 5]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring='accuracy', verbose=0)\n",
      "\n",
      "GridSearchCV 최적 하이퍼 파라미터:  {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "GridSearchCV 최고 정확도: 0.8070\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=0, splitter='best')\n",
      "\n",
      "dt 예측 정확도:  0.7938931297709924\n",
      "\n",
      "오차 행렬\n",
      "[[146  16]\n",
      " [ 38  62]]\n",
      "\n",
      "정확도: 0.7939, 정밀도: 0.7949, 재현율: 0.6200, F1: 0.6966\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV\n",
    "# DT 파라미터\n",
    "# max_depth: 트리의 최대 깊이\n",
    "# max_features: 최적의 분할을 위해 고려할 최대 피처 개수\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "parameters = {'max_depth': [2, 3, 5, 10]\n",
    "              , 'min_samples_split' : [2, 3, 5]\n",
    "              , 'min_samples_leaf': [1, 5, 8]}\n",
    "\n",
    "grid_dclf = GridSearchCV(dt_clf\n",
    "                         # 매개변수들\n",
    "                         , param_grid=parameters\n",
    "                         # 판정하기 위한 기준\n",
    "                         , scoring = 'accuracy'\n",
    "                         # 교차 검증 횟수\n",
    "                         , cv=5\n",
    "                         # 하이퍼 파라미터 적용 여부\n",
    "                         , refit=True)\n",
    "\n",
    "grid_dclf.fit(x_train, y_train)\n",
    "\n",
    "print(grid_dclf); print()\n",
    "# 교차 검증을 기반으로 최적의 하이퍼 파라미터를 찾아줌('max_depth', 'min_samples_split' , 'min_samples_leaf')\n",
    "print('GridSearchCV 최적 하이퍼 파라미터: ', grid_dclf.best_params_); print()\n",
    "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_)); print()\n",
    "\n",
    "# 최적의 하이퍼 파라미터 적용\n",
    "best_dclf = grid_dclf.best_estimator_\n",
    "print(best_dclf); print()\n",
    "\n",
    "dt_pred = best_dclf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, dt_pred)\n",
    "print('dt 예측 정확도: ', accuracy); print()\n",
    "\n",
    "# 평가 사용자 함수(재사용)\n",
    "get_clf_eval(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 한글 텍스트 처리 - 감성분석(네이버 영화평점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news_df = pd.read_csv('./dataset/nsmc/ratings_train.txt', sep='\\t')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 2)\n",
      "(150000,)\n"
     ]
    }
   ],
   "source": [
    "x = news_df.iloc[:, :-1]\n",
    "y = news_df.iloc[:, -1]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y\n",
    "    , test_size=0.2\n",
    "    , random_state=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120000 entries, 94561 to 141209\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        120000 non-null  int64 \n",
      " 1   document  119996 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null, 숫자를 공백으로 처리\n",
    "\n",
    "import re\n",
    "x_train = x_train.fillna(\" \")\n",
    "# 숫자로 된 거는 공백으로 채우기\n",
    "x_train.document = x_train.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
    "x_test = x_test.fillna(\" \")\n",
    "x_test.document = x_test.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120000 entries, 94561 to 141209\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        120000 non-null  int64 \n",
      " 1   document  120000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.7+ MB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 77509 to 36912\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        30000 non-null  int64 \n",
      " 1   document  30000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 703.1+ KB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()\n",
    "print()\n",
    "x_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphs() 메소드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화하여 list로 변환\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 TfidfVectorizer를 이용, TF-IDF 피처 모델을 생성(10분 소요)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 위에서 만든 tw_tokenizer() 함수를 tokenizer로 사용, ngram_range는 (1,2)\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    tokenizer=tw_tokenizer\n",
    "    , ngram_range=(1,2)\n",
    "    , min_df=3\n",
    "    # 상위 10% 피처로 추출하지 않음\n",
    "    , max_df=0.9\n",
    ")\n",
    "\n",
    "tfidf_vect.fit(x_train.document)\n",
    "tfidf_train = tfidf_vect.transform(x_train.document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 102036)\t0.3488783104460857\n",
      "  (0, 102033)\t0.2867925723448441\n",
      "  (0, 99699)\t0.35898185924840104\n",
      "  (0, 99578)\t0.156410381647359\n",
      "  (0, 82113)\t0.2619621255786964\n",
      "  (0, 72653)\t0.3460449458772446\n",
      "  (0, 72380)\t0.14773047063061884\n",
      "  (0, 55844)\t0.4014184120090594\n",
      "  (0, 55809)\t0.23072870024732475\n",
      "  (0, 22022)\t0.3630766943625821\n",
      "  (0, 22012)\t0.28218461994015404\n",
      "  (0, 1726)\t0.08173671204690627\n",
      "  (1, 95958)\t0.44802116798412084\n",
      "  (1, 95881)\t0.37841440380104724\n",
      "  (1, 75995)\t0.4522252939464176\n",
      "  (1, 75115)\t0.2460398749157744\n",
      "  (1, 67713)\t0.4918058520573429\n",
      "  (1, 67497)\t0.2010848328949181\n",
      "  (1, 26141)\t0.3297394871141796\n",
      "  (2, 100867)\t0.1936357004391474\n",
      "  (2, 100777)\t0.09460370403338593\n",
      "  (2, 98769)\t0.22716730077770947\n",
      "  (2, 98757)\t0.1522196211666142\n",
      "  (2, 95193)\t0.12421136973442137\n",
      "  (2, 94561)\t0.1312911155128571\n",
      "  :\t:\n",
      "  (119998, 35285)\t0.05180303746392642\n",
      "  (119998, 35049)\t0.1605763524069863\n",
      "  (119998, 34970)\t0.10664600869307865\n",
      "  (119998, 32045)\t0.09434841989838277\n",
      "  (119998, 31800)\t0.04996589191544733\n",
      "  (119998, 31441)\t0.07056382933706659\n",
      "  (119998, 31365)\t0.14078474164746158\n",
      "  (119998, 30590)\t0.1477071373280122\n",
      "  (119998, 29284)\t0.17128611641650535\n",
      "  (119998, 28946)\t0.09865319660674728\n",
      "  (119998, 10592)\t0.08939134951517147\n",
      "  (119998, 10503)\t0.06933726932665328\n",
      "  (119998, 8579)\t0.16220426966649967\n",
      "  (119998, 8176)\t0.04392629297539529\n",
      "  (119998, 7899)\t0.10636592386183222\n",
      "  (119998, 7830)\t0.09570248949249985\n",
      "  (119999, 82615)\t0.2347366371017079\n",
      "  (119999, 46457)\t0.42647312576970303\n",
      "  (119999, 46448)\t0.2920930105729466\n",
      "  (119999, 39983)\t0.37505295697213403\n",
      "  (119999, 39937)\t0.24569635305968973\n",
      "  (119999, 19581)\t0.3378420911608098\n",
      "  (119999, 19557)\t0.25265588006356904\n",
      "  (119999, 6980)\t0.19982712533754207\n",
      "  (119999, 5499)\t0.5086909539407876\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
