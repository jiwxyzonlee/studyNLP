{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 타이타닉 생존율 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   pclass    1309 non-null   int64  \n",
      " 1   survived  1309 non-null   int64  \n",
      " 2   sex       1309 non-null   int32  \n",
      " 3   age       1309 non-null   float64\n",
      " 4   sibsp     1309 non-null   int64  \n",
      " 5   parch     1309 non-null   int64  \n",
      " 6   fare      1309 non-null   float64\n",
      " 7   cabin     1309 non-null   int32  \n",
      " 8   embarked  1309 non-null   int32  \n",
      "dtypes: float64(2), int32(3), int64(4)\n",
      "memory usage: 76.8 KB\n"
     ]
    }
   ],
   "source": [
    "t_df = pd.read_pickle('./dataset/t_df.pkl')\n",
    "t_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9751671442215855\n",
      "\n",
      "예측 정확도 0.7595419847328244\n"
     ]
    }
   ],
   "source": [
    "y_df = t_df.survived\n",
    "x_df = t_df.drop('survived', axis = 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_df\n",
    "                                                    , y_df\n",
    "                                                    , test_size=0.2\n",
    "                                                    , random_state=0)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "dt_clf.fit(x_train, y_train)\n",
    "dt_pred = dt_clf.predict(x_test)\n",
    "\n",
    "print('Score: {}'.format(dt_clf.score(x_train, y_train)))\n",
    "print()\n",
    "\n",
    "df_accuracy = accuracy_score(y_test, dt_pred)\n",
    "print('예측 정확도', df_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 예측 정확도:  0.7938931297709924\n",
      "lr_clf.score(x_test, y_test)):  0.7938931297709924\n",
      "\n",
      "[train_score]:  0.789875835721108\n",
      "[test_score]:  0.7938931297709924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\nlp_python\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(random_state=0)\n",
    "\n",
    "lr_clf.fit(x_train, y_train)\n",
    "\n",
    "lr_pred = lr_clf.predict(x_test)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "print('lr 예측 정확도: ', lr_accuracy)\n",
    "\n",
    "# 아래와 같은 값임\n",
    "print('lr_clf.score(x_test, y_test)): ', lr_clf.score(x_test, y_test)); print()\n",
    "\n",
    "print('[train_score]: ', lr_clf.score(x_train, y_train))\n",
    "print('[test_score]: ', lr_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 사용자 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오차 행렬\n",
      "[[135  27]\n",
      " [ 36  64]]\n",
      "\n",
      "정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n오차 행렬 (생존자 예측이므로 1을 기준으로 하기 때문에 TP와 TN이 뒤집어짐)\\n예측---N---------P\\n실N[[135(TN)  27(FP)]\\n제P [ 36(FN)  64(TP)]]\\n\\n정밀도 = TP / (TP + FP)\\n재현율 = TP / (TP + FN)\\nf1 = 2 * {정밀도 * 재현율 / (정밀도 + 재현율)}\\n\\n정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가 사용자 함수\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    print('오차 행렬')\n",
    "    print(confusion)\n",
    "    print()\n",
    "    print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}'.format(accuracy, precision, recall, f1))\n",
    "\n",
    "get_clf_eval(y_test, dt_pred)\n",
    "\n",
    "\"\"\"\n",
    "오차 행렬 (생존자 예측이므로 1을 기준으로 하기 때문에 TP와 TN이 뒤집어짐)\n",
    "예측---N---------P\n",
    "실N[[135(TN)  27(FP)]\n",
    "제P [ 36(FN)  64(TP)]]\n",
    "\n",
    "정밀도 = TP / (TP + FP)\n",
    "재현율 = TP / (TP + FN)\n",
    "f1 = 2 * {정밀도 * 재현율 / (정밀도 + 재현율)}\n",
    "\n",
    "정확도: 0.7595, 정밀도: 0.7033, 재현율: 0.6400, F1: 0.6702\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 교차 검증 - KFold\n",
    "- 문제가 많아서 제한적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screenshot](./images/grid_search_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using  of the folds as training data;\n",
    "\n",
    "the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![screenshot](./images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증 0, 정확도: 0.5802\n",
      "교차 검증 1, 정확도: 0.7863\n",
      "교차 검증 2, 정확도: 0.8092\n",
      "교차 검증 3, 정확도: 0.7710\n",
      "교차 검증 4, 정확도: 0.7252\n",
      "교차 검증 5, 정확도: 0.7176\n",
      "교차 검증 6, 정확도: 0.6641\n",
      "교차 검증 7, 정확도: 0.6031\n",
      "교차 검증 8, 정확도: 0.6947\n",
      "교차 검증 9, 정확도: 0.7231\n",
      "\n",
      "평균 정확도: 0.7074\n"
     ]
    }
   ],
   "source": [
    "# cross_val_scores\n",
    "# KFold의 일련 과정을 한꺼번에 수행해주는 API\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(dt_clf, x_df, y_df, cv=10)\n",
    "for iter_count, accuracy in enumerate(scores):\n",
    "    print('교차 검증 {0}, 정확도: {1:.4f}'.format(iter_count, accuracy))\n",
    "\n",
    "print()\n",
    "print('평균 정확도: {0:.4f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "- CV 종합편"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score=nan,\n",
      "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
      "                                              criterion='gini', max_depth=None,\n",
      "                                              max_features=None,\n",
      "                                              max_leaf_nodes=None,\n",
      "                                              min_impurity_decrease=0.0,\n",
      "                                              min_impurity_split=None,\n",
      "                                              min_samples_leaf=1,\n",
      "                                              min_samples_split=2,\n",
      "                                              min_weight_fraction_leaf=0.0,\n",
      "                                              presort='deprecated',\n",
      "                                              random_state=0, splitter='best'),\n",
      "             iid='deprecated', n_jobs=None,\n",
      "             param_grid={'max_depth': [2, 3, 5, 10],\n",
      "                         'min_samples_leaf': [1, 5, 8],\n",
      "                         'min_samples_split': [2, 3, 5]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring='accuracy', verbose=0)\n",
      "\n",
      "GridSearchCV 최적 하이퍼 파라미터:  {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "\n",
      "GridSearchCV 최고 정확도: 0.8070\n",
      "\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=0, splitter='best')\n",
      "\n",
      "dt 예측 정확도:  0.7938931297709924\n",
      "\n",
      "오차 행렬\n",
      "[[146  16]\n",
      " [ 38  62]]\n",
      "\n",
      "정확도: 0.7939, 정밀도: 0.7949, 재현율: 0.6200, F1: 0.6966\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV\n",
    "# DT 파라미터\n",
    "# max_depth: 트리의 최대 깊이\n",
    "# max_features: 최적의 분할을 위해 고려할 최대 피처 개수\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "parameters = {'max_depth': [2, 3, 5, 10]\n",
    "              , 'min_samples_split' : [2, 3, 5]\n",
    "              , 'min_samples_leaf': [1, 5, 8]}\n",
    "\n",
    "grid_dclf = GridSearchCV(dt_clf\n",
    "                         # 매개변수들\n",
    "                         , param_grid=parameters\n",
    "                         # 판정하기 위한 기준\n",
    "                         , scoring = 'accuracy'\n",
    "                         # 교차 검증 횟수\n",
    "                         , cv=5\n",
    "                         # 하이퍼 파라미터 적용 여부\n",
    "                         , refit=True)\n",
    "\n",
    "grid_dclf.fit(x_train, y_train)\n",
    "\n",
    "print(grid_dclf); print()\n",
    "# 교차 검증을 기반으로 최적의 하이퍼 파라미터를 찾아줌('max_depth', 'min_samples_split' , 'min_samples_leaf')\n",
    "print('GridSearchCV 최적 하이퍼 파라미터: ', grid_dclf.best_params_); print()\n",
    "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_)); print()\n",
    "\n",
    "# 최적의 하이퍼 파라미터 적용\n",
    "best_dclf = grid_dclf.best_estimator_\n",
    "print(best_dclf); print()\n",
    "\n",
    "dt_pred = best_dclf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, dt_pred)\n",
    "print('dt 예측 정확도: ', accuracy); print()\n",
    "\n",
    "# 평가 사용자 함수(재사용)\n",
    "get_clf_eval(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 한글 텍스트 처리 - 감성분석(네이버 영화평점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "news_df = pd.read_csv('./dataset/nsmc/ratings_train.txt', sep='\\t')\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    75173\n",
       "1    74827\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150000, 2)\n",
      "(150000,)\n"
     ]
    }
   ],
   "source": [
    "x = news_df.iloc[:, :-1]\n",
    "y = news_df.iloc[:, -1]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y\n",
    "    , test_size=0.2\n",
    "    , random_state=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120000 entries, 94561 to 141209\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        120000 non-null  int64 \n",
      " 1   document  119996 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null, 숫자를 공백으로 처리\n",
    "\n",
    "import re\n",
    "x_train = x_train.fillna(\" \")\n",
    "# 숫자로 된 거는 공백으로 채우기\n",
    "x_train.document = x_train.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))\n",
    "x_test = x_test.fillna(\" \")\n",
    "x_test.document = x_test.document.apply(lambda x : re.sub(r\"\\d+\", \" \", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 120000 entries, 94561 to 141209\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   id        120000 non-null  int64 \n",
      " 1   document  120000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.7+ MB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 77509 to 36912\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        30000 non-null  int64 \n",
      " 1   document  30000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 703.1+ KB\n"
     ]
    }
   ],
   "source": [
    "x_train.info()\n",
    "print()\n",
    "x_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morphs() 메소드는 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화하여 list로 변환\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런의 TfidfVectorizer를 이용, TF-IDF 피처 모델을 생성(10분 소요)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 위에서 만든 tw_tokenizer() 함수를 tokenizer로 사용, ngram_range는 (1,2)\n",
    "tfidf_vect = TfidfVectorizer(\n",
    "    tokenizer=tw_tokenizer\n",
    "    , ngram_range=(1,2)\n",
    "    , min_df=3\n",
    "    # 상위 10% 피처로 추출하지 않음\n",
    "    , max_df=0.9\n",
    ")\n",
    "\n",
    "tfidf_vect.fit(x_train.document)\n",
    "tfidf_train = tfidf_vect.transform(x_train.document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.5} 0.8555\n"
     ]
    }
   ],
   "source": [
    "# 교차검증 및 하이퍼 파라미터 튜닝\n",
    "# Logistic Regression을 이용하여 감성 분석 Classification 수행\n",
    "# alpha 값의 역수(값이 작을수록 규제가 강한 것)\n",
    "# 로지스틱 회귀의 하이퍼 파라미터 C를 설정\n",
    "# C는 규제 강도를 조절하는  alpha 값의 역수로 작을수록 규제강도가 크며\n",
    "# Penalty는 규제의 유형을 설정하며 11 규제와 12규제가 있으며 기본은 12임\n",
    "lr_clf = LogisticRegression(random_state=0)\n",
    "params = {'C': [1,3.5,4.5,5.5,10]}\n",
    "\n",
    "gcv_lr = GridSearchCV(\n",
    "    lr_clf\n",
    "    , param_grid=params\n",
    "    , cv=3\n",
    "    , scoring='accuracy'\n",
    "    , verbose=0\n",
    ")\n",
    "\n",
    "gcv_lr.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_lr.best_params_, round(gcv_lr.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 정확도:  0.859\n"
     ]
    }
   ],
   "source": [
    "# 테스트 검증을 가지고 최종 검증\n",
    "# 테스트 세트를 이용 예측 시 학습할 때 적용한 TfidfVectorizer를 그대로 사용해야 함\n",
    "# 그래야 학습 시 설정한 피처 개수와 테스트 데이터를 TfidfVectorizer로 변경할 피처 개수가 같아짐\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_test = tfidf_vect.transform(x_test.document)\n",
    "\n",
    "best_estimator = gcv_lr.best_estimator_\n",
    "\n",
    "lr_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('Logistic Regression 정확도: ', accuracy_score(y_test, lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 1 1]\n",
      "[0 0 1 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test.values[:10])\n",
    "print(lr_preds[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. RF, DT를 이용하여 네이버 영화 평점 감성 분석 Classification 수행\n",
    "- GridSearchCV를 이용 교차검증(cv=3)과 하이퍼 파라미터 튜닝\n",
    " - RF params = {'n_estimators':[50,100,200], 'max_depth':[2,3,5], 'min_samples_leaf':[1,5,8]}\n",
    " - DT params = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'min_samples_leaf': 5, 'n_estimators': 200} 0.7538\n"
     ]
    }
   ],
   "source": [
    "# rf를 이용하여 감성 분석 Classification 수행\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf_params = {\n",
    "    # 나무 개수\n",
    "    'n_estimators':[50, 100, 200]\n",
    "    , 'max_depth':[2, 3, 5]\n",
    "    , 'min_samples_leaf':[1, 5, 8]}\n",
    "\n",
    "gcv_rf = GridSearchCV(rf_clf\n",
    "                      # 매개변수들\n",
    "                      , param_grid=rf_params\n",
    "                      # 판정하기 위한 기준\n",
    "                      , scoring = 'accuracy'\n",
    "                      # 교차 검증 횟수\n",
    "                      , cv=3\n",
    "                      , verbose=1)\n",
    "\n",
    "gcv_rf.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_rf.best_params_, round(gcv_rf.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest 정확도:  0.7545\n",
      "[0 0 1 0 0 0 0 1 1 1]\n",
      "[0 0 0 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 검증을 가지고 최종 검증\n",
    "# 테스트 세트를 이용 예측 시 학습할 때 적용한 TfidfVectorizer를 그대로 사용해야 함\n",
    "# 그래야 학습 시 설정한 피처 개수와 테스트 데이터를 TfidfVectorizer로 변경할 피처 개수가 같아짐\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_test = tfidf_vect.transform(x_test.document)\n",
    "best_estimator = gcv_rf.best_estimator_\n",
    "rf_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('RandomForest 정확도: ', accuracy_score(y_test, rf_preds))\n",
    "\n",
    "print(y_test.values[:10])\n",
    "print(rf_preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt를 이용하여 감성 분석 Classification 수행\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth':[2,3,5,10]\n",
    "    , 'max_depth':[2, 3, 5]\n",
    "    , 'min_samples_leaf':[1, 5, 8]}\n",
    "\n",
    "gcv_dt = GridSearchCV(dt_clf\n",
    "                      # 매개변수들\n",
    "                      , param_grid=dt_params\n",
    "                      # 판정하기 위한 기준\n",
    "                      , scoring = 'accuracy'\n",
    "                      # 교차 검증 횟수\n",
    "                      , cv=3\n",
    "                      , verbose=0)\n",
    "\n",
    "gcv_dt.fit(tfidf_train, y_train)\n",
    "\n",
    "print(gcv_dt.best_params_, round(gcv_dt.best_score_, 4))\n",
    "\n",
    "tfidf_test = tfidf_vect.transform(x_test.document)\n",
    "best_estimator = gcv_dt.best_estimator_\n",
    "dt_preds = best_estimator.predict(tfidf_test)\n",
    "\n",
    "print('Decision Tree 정확도: ', accuracy_score(y_test, dt_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
